{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2\n",
    "\n",
    "# Python \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Custom Packages\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2150de2-7c1e-4645-9f29-eec02c6618a2",
   "metadata": {},
   "source": [
    "## 0.1 File paths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdded948-6a02-4ace-9948-0ebedf00120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_main = Path('/Volumes/SAM-SODAS-DISTRACT/Coding Distraction/github_as_a_market_device')\n",
    "fp_main_output = Path(fp_main / 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping users from the initial company list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Processing the company list into:\n",
    "1. Dictionary containing company categories --> company_category\n",
    "2. A zip_file containing the company-string to query when scraping plus a location_filter dummy, indicating whether the location filter should apply when queriyng that company --> user_location_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the company list from Excel\n",
    "company_list_full = pd.read_excel(\n",
    "    fp_main / 'company_list' / 'company_info_list211022.xlsx',\n",
    "    usecols='A:M'\n",
    ")\n",
    "\n",
    "# Filter for companies that are part of our sample\n",
    "company_list_sample = company_list_full.loc[\n",
    "    company_list_full['part_of_firmaliste'] == 1\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Create a list of relevant company info\n",
    "list_of_company_names = company_list_sample[\n",
    "    ['søgeord', 'new_company_category', 'uden lokation']\n",
    "]\n",
    "\n",
    "# Create a dictionary mapping company name (lowercase) to category\n",
    "company_category = dict(zip(\n",
    "    company_list_sample['søgeord'].str.lower(),\n",
    "    company_list_sample['new_company_category']\n",
    "))\n",
    "\n",
    "# Write the dictionary as JSONL\n",
    "with open(fp_main_output / 'company_category_dict.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for company, category in company_category.items():\n",
    "        json.dump({'søgeord': company, 'new_company_category': category}, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip object for name and location filter status\n",
    "company_location_filter_bool_zip = zip(\n",
    "    list_of_company_names['søgeord'],\n",
    "    list_of_company_names['uden lokation'],\n",
    "    company_list_sample['company_label_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Queriyng the company names and scraping users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in logs in case scrape has been interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User log exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] Company log exists: first_tier_userinfo_company_log.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Collecting users already scraped\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "\n",
    "user_log_file_name = 'first_tier_userinfo_user_log.jsonl'\n",
    "company_log_file_name = 'first_tier_userinfo_company_log.jsonl'\n",
    "user_log_path = fp_main_output / user_log_file_name\n",
    "company_log_path = fp_main_output / company_log_file_name\n",
    "\n",
    "# Ensure files exist and print message\n",
    "if user_log_path.exists():\n",
    "    print(f\"[INFO] User log exists: {user_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] User log does NOT exist. Creating new file: {user_log_path.name}\")\n",
    "    user_log_path.touch(exist_ok=True)\n",
    "\n",
    "if company_log_path.exists():\n",
    "    print(f\"[INFO] Company log exists: {company_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] Company log does NOT exist. Creating new file: {company_log_path.name}\")\n",
    "    company_log_path.touch(exist_ok=True)\n",
    "\n",
    "# Read user log\n",
    "with open(user_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            user_info = json.loads(line)\n",
    "            users_already_scraped.add(user_info['user_login'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed user line: {err}\")\n",
    "\n",
    "# Read company log\n",
    "with open(company_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            company = json.loads(line)\n",
    "            companies_already_scraped.add(company['company_name'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed company line: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 3 tokens.\n",
      "First token in cycle. Initiating ACCESS_TOKEN_1.\n",
      "GithubScraper initialized with 53 companies and 223 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token ACCESS_TOKEN_1 is 2025-06-02 11:55:13. That will be in a little less than 37 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Company nodes already scraped. Skipping.\n",
      "[INFO] Company abtion already scraped. Skipping.\n",
      "[INFO] Company heyday already scraped. Skipping.\n",
      "[INFO] Company trifork already scraped. Skipping.\n",
      "[INFO] Company frontit already scraped. Skipping.\n",
      "[INFO] Company holion already scraped. Skipping.\n",
      "[INFO] Company kruso already scraped. Skipping.\n",
      "[INFO] Company pandiweb already scraped. Skipping.\n",
      "[INFO] Company uptime already scraped. Skipping.\n",
      "[INFO] Company charlie tango already scraped. Skipping.\n",
      "[INFO] Company ffw already scraped. Skipping.\n",
      "[INFO] Company mysupport already scraped. Skipping.\n",
      "[INFO] Company shape already scraped. Skipping.\n",
      "[INFO] Company makeable already scraped. Skipping.\n",
      "[INFO] Company mustache already scraped. Skipping.\n",
      "[INFO] Company house of code already scraped. Skipping.\n",
      "[INFO] Company greener pastures already scraped. Skipping.\n",
      "[INFO] Company axla already scraped. Skipping.\n",
      "[INFO] Company snapp already scraped. Skipping.\n",
      "[INFO] Company appscaptain already scraped. Skipping.\n",
      "[INFO] Company adtomic already scraped. Skipping.\n",
      "[INFO] Company signifly already scraped. Skipping.\n",
      "[INFO] Company creuna already scraped. Skipping.\n",
      "[INFO] Company strømlin already scraped. Skipping.\n",
      "[INFO] Company knowit already scraped. Skipping.\n",
      "[INFO] Company must already scraped. Skipping.\n",
      "[INFO] Company netcompany already scraped. Skipping.\n",
      "[INFO] Company systematic already scraped. Skipping.\n",
      "[INFO] Company capgemini already scraped. Skipping.\n",
      "[INFO] Company sas institute already scraped. Skipping.\n",
      "[INFO] Company fellowmind already scraped. Skipping.\n",
      "[INFO] Company eg a/s already scraped. Skipping.\n",
      "[INFO] Company kmd already scraped. Skipping.\n",
      "[INFO] Company adform already scraped. Skipping.\n",
      "[INFO] Company oxygen already scraped. Skipping.\n",
      "[INFO] Company saxo bank already scraped. Skipping.\n",
      "[INFO] Company kabellmunk already scraped. Skipping.\n",
      "[INFO] Company dgi-it already scraped. Skipping.\n",
      "[INFO] Company ørsted already scraped. Skipping.\n",
      "[INFO] Company nuuday already scraped. Skipping.\n",
      "[INFO] Company yousee already scraped. Skipping.\n",
      "[INFO] Company relatel already scraped. Skipping.\n",
      "[INFO] Company cphapp already scraped. Skipping.\n",
      "[INFO] Company commentor already scraped. Skipping.\n",
      "[INFO] Company nabto already scraped. Skipping.\n",
      "[INFO] Company jobindex already scraped. Skipping.\n",
      "[INFO] Company miracle already scraped. Skipping.\n",
      "[INFO] Company immeo already scraped. Skipping.\n",
      "[INFO] Company siteimprove already scraped. Skipping.\n",
      "[INFO] Company cbrain already scraped. Skipping.\n",
      "[INFO] Company deon digital already scraped. Skipping.\n",
      "[INFO] Company pwc already scraped. Skipping.\n",
      "[INFO] Company studiesandme already scraped. Skipping.\n",
      "[INFO] Scraping users for company: tv2\n",
      "[INFO] User tv2 already scraped. Skipping.\n",
      "[INFO] Scraping user: quartercastle\n",
      "User match quartercastle logged.\n",
      "[INFO] 224 users scraped so far.\n",
      "[INFO] Scraping user: smukkejohan\n",
      "User match smukkejohan logged.\n",
      "[INFO] 225 users scraped so far.\n",
      "[INFO] Scraping user: tv2regionerne\n",
      "User match tv2regionerne logged.\n",
      "[INFO] 226 users scraped so far.\n",
      "[INFO] Scraping user: tv2-oss\n",
      "User match tv2-oss logged.\n",
      "[INFO] 227 users scraped so far.\n",
      "[INFO] Scraping user: LasseEmilHildebrandt\n",
      "User match LasseEmilHildebrandt logged.\n",
      "[INFO] 228 users scraped so far.\n",
      "[INFO] Scraping user: TV2-Fyn\n",
      "User match TV2-Fyn logged.\n",
      "[INFO] 229 users scraped so far.\n",
      "[INFO] Scraping user: MartinDreyer\n",
      "User match MartinDreyer logged.\n",
      "[INFO] 230 users scraped so far.\n",
      "[INFO] Scraping user: tv2-pvin\n",
      "User match tv2-pvin logged.\n",
      "[INFO] 231 users scraped so far.\n",
      "[INFO] Scraping user: radziko\n",
      "User match radziko logged.\n",
      "[INFO] 232 users scraped so far.\n",
      "[INFO] Scraping user: MahaEast\n",
      "User match MahaEast logged.\n",
      "[INFO] 233 users scraped so far.\n",
      "[INFO] Scraping user: tv2-sile\n",
      "User match tv2-sile logged.\n",
      "[INFO] 234 users scraped so far.\n",
      "[INFO] Scraping user: tv2bornholm\n",
      "User match tv2bornholm logged.\n",
      "[INFO] 235 users scraped so far.\n",
      "[INFO] Scraping user: tv2net\n",
      "User match tv2net logged.\n",
      "[INFO] 236 users scraped so far.\n",
      "[INFO] Scraping user: tv2oest\n",
      "User match tv2oest logged.\n",
      "[INFO] 237 users scraped so far.\n",
      "[INFO] Scraping user: TV2R\n",
      "User match TV2R logged.\n",
      "[INFO] 238 users scraped so far.\n",
      "[INFO] Scraping user: ahorTV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 54/64 [14:38<02:42, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match ahorTV2 logged.\n",
      "[INFO] 239 users scraped so far.\n",
      "Company tv2 logged.\n",
      "[INFO] Scraping users for company: pentia\n",
      "[INFO] Scraping user: JakobChristensen\n",
      "User match JakobChristensen logged.\n",
      "[INFO] 240 users scraped so far.\n",
      "[INFO] Scraping user: PentiaLabs\n",
      "User match PentiaLabs logged.\n",
      "[INFO] 241 users scraped so far.\n",
      "[INFO] Scraping user: pentia-mobile\n",
      "User match pentia-mobile logged.\n",
      "[INFO] 242 users scraped so far.\n",
      "[INFO] Scraping user: chhoejgaard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 55/64 [22:06<04:06, 27.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match chhoejgaard logged.\n",
      "[INFO] 243 users scraped so far.\n",
      "Company pentia logged.\n",
      "[INFO] Scraping users for company: zervme\n",
      "[INFO] Scraping user: ZervMe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 56/64 [22:11<03:32, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match ZervMe logged.\n",
      "[INFO] 244 users scraped so far.\n",
      "Company zervme logged.\n",
      "[INFO] Scraping users for company: skat\n",
      "[INFO] Scraping user: skat\n",
      "Token cycled to ACCESS_TOKEN_2.\n",
      "Cycle\n",
      "User match skat logged.\n",
      "[INFO] 245 users scraped so far.\n",
      "[INFO] Scraping user: skat-lab\n",
      "User match skat-lab logged.\n",
      "[INFO] 246 users scraped so far.\n",
      "[INFO] Scraping user: skat-lj\n",
      "User match skat-lj logged.\n",
      "[INFO] 247 users scraped so far.\n",
      "[INFO] Scraping user: nc-tpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /users/nc-tpe/followers failed with 403: Forbidden\n",
      "Setting next backoff to 622.094251s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match nc-tpe logged.\n",
      "[INFO] 248 users scraped so far.\n",
      "[INFO] Scraping user: skatvaamsi\n",
      "User match skatvaamsi logged.\n",
      "[INFO] 249 users scraped so far.\n",
      "[INFO] Scraping user: Skatteministeriet\n",
      "[INFO] Scraping user: nc-law\n",
      "User match nc-law logged.\n",
      "[INFO] 250 users scraped so far.\n",
      "[INFO] Scraping user: nc-llh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 57/64 [36:25<07:45, 66.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match nc-llh logged.\n",
      "[INFO] 251 users scraped so far.\n",
      "Company skat logged.\n",
      "[INFO] Scraping users for company: codefort\n",
      "[INFO] Scraping user: CodeForTravel\n",
      "User match CodeForTravel logged.\n",
      "[INFO] 252 users scraped so far.\n",
      "[INFO] Scraping user: codefort-io\n",
      "User match codefort-io logged.\n",
      "[INFO] 253 users scraped so far.\n",
      "[INFO] Scraping user: codefortbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 58/64 [37:50<06:46, 67.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match codefortbot logged.\n",
      "[INFO] 254 users scraped so far.\n",
      "Company codefort logged.\n",
      "[INFO] Scraping users for company: reepay\n",
      "[INFO] Scraping user: reepay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 59/64 [39:16<05:46, 69.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match reepay logged.\n",
      "[INFO] 255 users scraped so far.\n",
      "Company reepay logged.\n",
      "[INFO] Scraping users for company: diviso\n",
      "[INFO] User MySupport-aps already scraped. Skipping.\n",
      "[INFO] Scraping user: lassewq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [39:19<04:08, 62.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match lassewq logged.\n",
      "[INFO] 256 users scraped so far.\n",
      "Company diviso logged.\n",
      "[INFO] Scraping users for company: uni-soft\n",
      "[INFO] Scraping user: uni-soft\n",
      "[INFO] Scraping user: uni-software\n",
      "[INFO] Scraping user: UNI-Software-House\n",
      "[INFO] Scraping user: UNI-Software-II\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 61/64 [39:24<02:43, 54.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company uni-soft logged.\n",
      "[INFO] Scraping users for company: delegateas\n",
      "[INFO] Scraping user: delegateas\n",
      "User match delegateas logged.\n",
      "[INFO] 257 users scraped so far.\n",
      "[INFO] Scraping user: JonasGLund99\n",
      "User match JonasGLund99 logged.\n",
      "[INFO] 258 users scraped so far.\n",
      "[INFO] Scraping user: mkholt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 62/64 [49:16<04:41, 140.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match mkholt logged.\n",
      "[INFO] 259 users scraped so far.\n",
      "Company delegateas logged.\n",
      "[INFO] Scraping users for company: proactivedk\n",
      "[INFO] Scraping user: proactivedk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 63/64 [49:22<01:55, 115.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match proactivedk logged.\n",
      "[INFO] 260 users scraped so far.\n",
      "Company proactivedk logged.\n",
      "[INFO] Scraping users for company: monstarlab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [49:23<00:00, 46.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company monstarlab logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    repo_limit=300\n",
    ")\n",
    "print(f'GitHub REST API ratelimit reset time for token {gs.current_token_key} is {gs.reset_time_point}. '\n",
    "      f'That will be in a little less than {gs.reset_time_in_minutes} minutes.')\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = 'first_tier_userinfo'\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for search_query, skip_location_filter, company_label in tqdm(company_location_filter_bool_zip, total=len(company_list_sample)):\n",
    "\n",
    "    # 3.1 Skip company if already scraped\n",
    "    if company_label in gs.companies_already_scraped:\n",
    "        print(f'[INFO] Company {company_label} already scraped. Skipping.')\n",
    "        continue\n",
    "\n",
    "    print(f'[INFO] Scraping users for company: {company_label}')\n",
    "    \n",
    "    # 3.2 Get users for this company\n",
    "    users = gs.get_gh_users(search_query, skip_location_filter)\n",
    "\n",
    "    # 3.3 Loop through users\n",
    "    for named_user, company in users:\n",
    "\n",
    "        # 3.3.1 Skip user if already scraped\n",
    "        if named_user.login in gs.users_already_scraped:\n",
    "            print(f'[INFO] User {named_user.login} already scraped. Skipping.')\n",
    "            continue\n",
    "\n",
    "        print(f'[INFO] Scraping user: {named_user.login}')\n",
    "        gs.users_already_scraped.add(named_user.login)\n",
    "\n",
    "        # 3.3.2 Get user info (may return None if repo limit exceeded or no match)\n",
    "        user_row = gs.get_user_info(named_user, company_label)\n",
    "        if user_row is None:\n",
    "            continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "        # 3.3.3 Extract match data\n",
    "        location_match = user_row.location\n",
    "        inferred_company = user_row.inferred_company\n",
    "        matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "        # 3.3.4 Save user info and log result\n",
    "        gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "        gs.log_user_w_match(named_user.login, inferred_company, matched_company_strings, location_match, user_log_path)\n",
    "        \n",
    "        print(f'[INFO] {gs.USERS_SCRAPED} users scraped so far.')\n",
    "\n",
    "    # 3.4 Log company after scraping all users\n",
    "    gs.log_company(company_label, company_log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
