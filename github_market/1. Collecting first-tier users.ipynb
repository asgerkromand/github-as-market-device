{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2\n",
    "\n",
    "# Python \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Custom Packages\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2150de2-7c1e-4645-9f29-eec02c6618a2",
   "metadata": {},
   "source": [
    "## 0.1 File paths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdded948-6a02-4ace-9948-0ebedf00120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_main = Path('/Volumes/SAM-SODAS-DISTRACT/Coding Distraction/github_as_a_market_device')\n",
    "fp_main_output = Path(fp_main / 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping users from the initial company list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Processing the company list into:\n",
    "1. Dictionary containing company categories --> company_category\n",
    "2. A zip_file containing the company-string to query when scraping plus a location_filter dummy, indicating whether the location filter should apply when queriyng that company --> user_location_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the company list from Excel\n",
    "company_list_full = pd.read_excel(\n",
    "    fp_main / 'company_list' / 'company_info_list211022.xlsx',\n",
    "    usecols='A:M'\n",
    ")\n",
    "\n",
    "# Filter for companies that are part of our sample\n",
    "company_list_sample = company_list_full.loc[\n",
    "    company_list_full['part_of_firmaliste'] == 1\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Create a list of relevant company info\n",
    "list_of_company_names = company_list_sample[\n",
    "    ['søgeord', 'new_company_category', 'uden lokation']\n",
    "]\n",
    "\n",
    "# Create a dictionary mapping company name (lowercase) to category\n",
    "company_category = dict(zip(\n",
    "    company_list_sample['søgeord'].str.lower(),\n",
    "    company_list_sample['new_company_category']\n",
    "))\n",
    "\n",
    "# Write the dictionary as JSONL\n",
    "with open(fp_main_output / 'company_category_dict.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for company, category in company_category.items():\n",
    "        json.dump({'søgeord': company, 'new_company_category': category}, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip object for name and location filter status\n",
    "company_location_filter_bool_zip = zip(\n",
    "    list_of_company_names['søgeord'],\n",
    "    list_of_company_names['uden lokation'],\n",
    "    company_list_sample['company_label_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Queriyng the company names and scraping users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in logs in case scrape has been interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User log exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] Company log exists: first_tier_userinfo_company_log.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Collecting users already scraped\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "\n",
    "user_log_file_name = 'first_tier_userinfo_user_log.jsonl'\n",
    "company_log_file_name = 'first_tier_userinfo_company_log.jsonl'\n",
    "user_log_path = fp_main_output / user_log_file_name\n",
    "company_log_path = fp_main_output / company_log_file_name\n",
    "\n",
    "# Ensure files exist and print message\n",
    "if user_log_path.exists():\n",
    "    print(f\"[INFO] User log exists: {user_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] User log does NOT exist. Creating new file: {user_log_path.name}\")\n",
    "    user_log_path.touch(exist_ok=True)\n",
    "\n",
    "if company_log_path.exists():\n",
    "    print(f\"[INFO] Company log exists: {company_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] Company log does NOT exist. Creating new file: {company_log_path.name}\")\n",
    "    company_log_path.touch(exist_ok=True)\n",
    "\n",
    "# Read user log\n",
    "with open(user_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            user_info = json.loads(line)\n",
    "            users_already_scraped.add(user_info['user_login'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed user line: {err}\")\n",
    "\n",
    "# Read company log\n",
    "with open(company_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            company = json.loads(line)\n",
    "            companies_already_scraped.add(company['company_name'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed company line: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 3 tokens.\n",
      "First token in cycle. Initiating ACCESS_TOKEN_1.\n",
      "GithubScraper initialized with 64 companies and 260 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token ACCESS_TOKEN_1 is 2025-06-10 12:03:52. That will be in a little less than 12 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 148553.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Company nodes already scraped. Skipping.\n",
      "[INFO] Company abtion already scraped. Skipping.\n",
      "[INFO] Company heyday already scraped. Skipping.\n",
      "[INFO] Company trifork already scraped. Skipping.\n",
      "[INFO] Company frontit already scraped. Skipping.\n",
      "[INFO] Company holion already scraped. Skipping.\n",
      "[INFO] Company kruso already scraped. Skipping.\n",
      "[INFO] Company pandiweb already scraped. Skipping.\n",
      "[INFO] Company uptime already scraped. Skipping.\n",
      "[INFO] Company charlie tango already scraped. Skipping.\n",
      "[INFO] Company ffw already scraped. Skipping.\n",
      "[INFO] Company mysupport already scraped. Skipping.\n",
      "[INFO] Company shape already scraped. Skipping.\n",
      "[INFO] Company makeable already scraped. Skipping.\n",
      "[INFO] Company mustache already scraped. Skipping.\n",
      "[INFO] Company house of code already scraped. Skipping.\n",
      "[INFO] Company greener pastures already scraped. Skipping.\n",
      "[INFO] Company axla already scraped. Skipping.\n",
      "[INFO] Company snapp already scraped. Skipping.\n",
      "[INFO] Company appscaptain already scraped. Skipping.\n",
      "[INFO] Company adtomic already scraped. Skipping.\n",
      "[INFO] Company signifly already scraped. Skipping.\n",
      "[INFO] Company creuna already scraped. Skipping.\n",
      "[INFO] Company strømlin already scraped. Skipping.\n",
      "[INFO] Company knowit already scraped. Skipping.\n",
      "[INFO] Company must already scraped. Skipping.\n",
      "[INFO] Company netcompany already scraped. Skipping.\n",
      "[INFO] Company systematic already scraped. Skipping.\n",
      "[INFO] Company capgemini already scraped. Skipping.\n",
      "[INFO] Company sas institute already scraped. Skipping.\n",
      "[INFO] Company fellowmind already scraped. Skipping.\n",
      "[INFO] Company eg a/s already scraped. Skipping.\n",
      "[INFO] Company kmd already scraped. Skipping.\n",
      "[INFO] Company adform already scraped. Skipping.\n",
      "[INFO] Company oxygen already scraped. Skipping.\n",
      "[INFO] Company saxo bank already scraped. Skipping.\n",
      "[INFO] Company kabellmunk already scraped. Skipping.\n",
      "[INFO] Company dgi-it already scraped. Skipping.\n",
      "[INFO] Company ørsted already scraped. Skipping.\n",
      "[INFO] Company nuuday already scraped. Skipping.\n",
      "[INFO] Company yousee already scraped. Skipping.\n",
      "[INFO] Company relatel already scraped. Skipping.\n",
      "[INFO] Company cphapp already scraped. Skipping.\n",
      "[INFO] Company commentor already scraped. Skipping.\n",
      "[INFO] Company nabto already scraped. Skipping.\n",
      "[INFO] Company jobindex already scraped. Skipping.\n",
      "[INFO] Company miracle already scraped. Skipping.\n",
      "[INFO] Company immeo already scraped. Skipping.\n",
      "[INFO] Company siteimprove already scraped. Skipping.\n",
      "[INFO] Company cbrain already scraped. Skipping.\n",
      "[INFO] Company deon digital already scraped. Skipping.\n",
      "[INFO] Company pwc already scraped. Skipping.\n",
      "[INFO] Company studiesandme already scraped. Skipping.\n",
      "[INFO] Company tv2 already scraped. Skipping.\n",
      "[INFO] Company pentia already scraped. Skipping.\n",
      "[INFO] Company zervme already scraped. Skipping.\n",
      "[INFO] Company skat already scraped. Skipping.\n",
      "[INFO] Company codefort already scraped. Skipping.\n",
      "[INFO] Company reepay already scraped. Skipping.\n",
      "[INFO] Company diviso already scraped. Skipping.\n",
      "[INFO] Company uni-soft already scraped. Skipping.\n",
      "[INFO] Company delegateas already scraped. Skipping.\n",
      "[INFO] Company proactivedk already scraped. Skipping.\n",
      "[INFO] Company monstarlab already scraped. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    repo_limit=300\n",
    ")\n",
    "print(f'GitHub REST API ratelimit reset time for token {gs.current_token_key} is {gs.reset_time_point}. '\n",
    "      f'That will be in a little less than {gs.reset_time_in_minutes} minutes.')\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = 'first_tier_userinfo'\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for search_query, skip_location_filter, company_label in tqdm(company_location_filter_bool_zip, total=len(company_list_sample)):\n",
    "\n",
    "    # 3.1 Skip company if already scraped\n",
    "    if company_label in gs.companies_already_scraped:\n",
    "        print(f'[INFO] Company {company_label} already scraped. Skipping.')\n",
    "        continue\n",
    "\n",
    "    print(f'[INFO] Scraping users for company: {company_label}')\n",
    "    \n",
    "    # 3.2 Get users for this company\n",
    "    users = gs.get_gh_users(search_query, skip_location_filter)\n",
    "\n",
    "    # 3.3 Loop through users\n",
    "    for named_user, company in users:\n",
    "\n",
    "        # 3.3.1 Skip user if already scraped\n",
    "        if named_user.login in gs.users_already_scraped:\n",
    "            print(f'[INFO] User {named_user.login} already scraped. Skipping.')\n",
    "            continue\n",
    "\n",
    "        print(f'[INFO] Scraping user: {named_user.login}')\n",
    "        gs.users_already_scraped.add(named_user.login)\n",
    "\n",
    "        # 3.3.2 Get user info (may return None if repo limit exceeded or no match)\n",
    "        user_row = gs.get_user_info(named_user, company_label)\n",
    "        if user_row is None:\n",
    "            continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "        # 3.3.3 Extract match data\n",
    "        location_match = user_row.location\n",
    "        inferred_company = user_row.inferred_company\n",
    "        matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "        # 3.3.4 Save user info and log result\n",
    "        gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "        gs.log_user_w_match(named_user.login, inferred_company, matched_company_strings, location_match, user_log_path)\n",
    "        \n",
    "        print(f'[INFO] {gs.USERS_SCRAPED} users scraped so far.')\n",
    "\n",
    "    # 3.4 Log company after scraping all users\n",
    "    gs.log_company(company_label, company_log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
