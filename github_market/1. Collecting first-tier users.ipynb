{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2\n",
    "\n",
    "# Python \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Custom Packages\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2150de2-7c1e-4645-9f29-eec02c6618a2",
   "metadata": {},
   "source": [
    "## 0.1 File paths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdded948-6a02-4ace-9948-0ebedf00120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_main = Path('/Volumes/SAM-SODAS-DISTRACT/Coding Distraction/github_as_a_market_device')\n",
    "fp_main_output = Path(fp_main / 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping users from the initial company list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Processing the company list into:\n",
    "1. Dictionary containing company categories --> company_category\n",
    "2. A zip_file containing the company-string to query when scraping plus a location_filter dummy, indicating whether the location filter should apply when queriyng that company --> user_location_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the company list from Excel\n",
    "company_list_full = pd.read_excel(\n",
    "    fp_main / 'company_list' / 'company_info_list211022.xlsx',\n",
    "    usecols='A:M'\n",
    ")\n",
    "\n",
    "# Filter for companies that are part of our sample\n",
    "company_list_sample = company_list_full.loc[\n",
    "    company_list_full['part_of_firmaliste'] == 1\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Create a list of relevant company info\n",
    "list_of_company_names = company_list_sample[\n",
    "    ['søgeord', 'new_company_category', 'uden lokation']\n",
    "]\n",
    "\n",
    "# Create a dictionary mapping company name (lowercase) to category\n",
    "company_category = dict(zip(\n",
    "    company_list_sample['søgeord'].str.lower(),\n",
    "    company_list_sample['new_company_category']\n",
    "))\n",
    "\n",
    "# Write the dictionary as JSONL\n",
    "with open(fp_main_output / 'company_category_dict.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for company, category in company_category.items():\n",
    "        json.dump({'søgeord': company, 'new_company_category': category}, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip object for name and location filter status\n",
    "company_location_filter_bool_zip = zip(\n",
    "    list_of_company_names['søgeord'],\n",
    "    list_of_company_names['uden lokation'],\n",
    "    company_list_sample['company_label_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Queriyng the company names and scraping users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in logs in case scrape has been interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User log exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] Company log exists: first_tier_userinfo_company_log.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Collecting users already scraped\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "\n",
    "user_log_file_name = 'first_tier_userinfo_user_log.jsonl'\n",
    "company_log_file_name = 'first_tier_userinfo_company_log.jsonl'\n",
    "user_log_path = fp_main_output / user_log_file_name\n",
    "company_log_path = fp_main_output / company_log_file_name\n",
    "\n",
    "# Ensure files exist and print message\n",
    "if user_log_path.exists():\n",
    "    print(f\"[INFO] User log exists: {user_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] User log does NOT exist. Creating new file: {user_log_path.name}\")\n",
    "    user_log_path.touch(exist_ok=True)\n",
    "\n",
    "if company_log_path.exists():\n",
    "    print(f\"[INFO] Company log exists: {company_log_path.name}\")\n",
    "else:\n",
    "    print(f\"[INFO] Company log does NOT exist. Creating new file: {company_log_path.name}\")\n",
    "    company_log_path.touch(exist_ok=True)\n",
    "\n",
    "# Read user log\n",
    "with open(user_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            user_info = json.loads(line)\n",
    "            users_already_scraped.add(user_info['user_login'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed user line: {err}\")\n",
    "\n",
    "# Read company log\n",
    "with open(company_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            company = json.loads(line)\n",
    "            companies_already_scraped.add(company['company_name'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed company line: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 3 tokens.\n",
      "First token in cycle. Initiating ACCESS_TOKEN_1.\n",
      "GithubScraper initialized with 35 companies and 185 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token ACCESS_TOKEN_1 is 2025-05-28 14:29:41. That will be in a little less than 30 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Company nodes already scraped. Skipping.\n",
      "[INFO] Company abtion already scraped. Skipping.\n",
      "[INFO] Company heyday already scraped. Skipping.\n",
      "[INFO] Company trifork already scraped. Skipping.\n",
      "[INFO] Company frontit already scraped. Skipping.\n",
      "[INFO] Company holion already scraped. Skipping.\n",
      "[INFO] Company kruso already scraped. Skipping.\n",
      "[INFO] Company pandiweb already scraped. Skipping.\n",
      "[INFO] Company uptime already scraped. Skipping.\n",
      "[INFO] Company charlie tango already scraped. Skipping.\n",
      "[INFO] Company ffw already scraped. Skipping.\n",
      "[INFO] Company mysupport already scraped. Skipping.\n",
      "[INFO] Company shape already scraped. Skipping.\n",
      "[INFO] Company makeable already scraped. Skipping.\n",
      "[INFO] Company mustache already scraped. Skipping.\n",
      "[INFO] Company house of code already scraped. Skipping.\n",
      "[INFO] Company greener pastures already scraped. Skipping.\n",
      "[INFO] Company axla already scraped. Skipping.\n",
      "[INFO] Company snapp already scraped. Skipping.\n",
      "[INFO] Company appscaptain already scraped. Skipping.\n",
      "[INFO] Company adtomic already scraped. Skipping.\n",
      "[INFO] Company signifly already scraped. Skipping.\n",
      "[INFO] Company creuna already scraped. Skipping.\n",
      "[INFO] Company strømlin already scraped. Skipping.\n",
      "[INFO] Company knowit already scraped. Skipping.\n",
      "[INFO] Company must already scraped. Skipping.\n",
      "[INFO] Company netcompany already scraped. Skipping.\n",
      "[INFO] Company systematic already scraped. Skipping.\n",
      "[INFO] Company capgemini already scraped. Skipping.\n",
      "[INFO] Company sas institute already scraped. Skipping.\n",
      "[INFO] Company fellowmind already scraped. Skipping.\n",
      "[INFO] Company eg a/s already scraped. Skipping.\n",
      "[INFO] Company kmd already scraped. Skipping.\n",
      "[INFO] Company adform already scraped. Skipping.\n",
      "[INFO] Company oxygen already scraped. Skipping.\n",
      "[INFO] Scraping users for company: saxo bank\n",
      "[INFO] Scraping user: PederHP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /users/elsewhat failed with 403: Forbidden\n",
      "Setting next backoff to 581.257467s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match PederHP logged.\n",
      "[INFO] 186 users scraped so far.\n",
      "[INFO] Scraping user: michelandresaxo\n",
      "User match michelandresaxo logged.\n",
      "[INFO] 187 users scraped so far.\n",
      "[INFO] Scraping user: jorgeta\n",
      "User match jorgeta logged.\n",
      "[INFO] 188 users scraped so far.\n",
      "[INFO] Scraping user: phccdk\n",
      "User match phccdk logged.\n",
      "[INFO] 189 users scraped so far.\n",
      "[INFO] Scraping user: hannefolmer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 36/64 [31:24<24:25, 52.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match hannefolmer logged.\n",
      "[INFO] 190 users scraped so far.\n",
      "Company saxo bank logged.\n",
      "[INFO] Scraping users for company: kabellmunk\n",
      "[INFO] Scraping user: kabellmunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 37/64 [31:27<22:42, 50.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match kabellmunk logged.\n",
      "[INFO] 191 users scraped so far.\n",
      "Company kabellmunk logged.\n",
      "[INFO] Scraping users for company: dgi-it\n",
      "[INFO] Scraping user: DGI-IT-zz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 38/64 [31:28<20:46, 47.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company dgi-it logged.\n",
      "[INFO] Scraping users for company: ørsted\n",
      "[INFO] Scraping user: HansOersted\n",
      "[INFO] Scraping user: Orsted\n",
      "User match Orsted logged.\n",
      "[INFO] 192 users scraped so far.\n",
      "[INFO] Scraping user: zlin\n",
      "[INFO] Scraping user: 0rsted\n",
      "[INFO] Scraping user: H-C-Orsted-Gym\n",
      "[INFO] Scraping user: Programmering-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 39/64 [31:42<18:59, 45.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company ørsted logged.\n",
      "[INFO] Scraping users for company: nuuday\n",
      "[INFO] Scraping user: nuuday\n",
      "User match nuuday logged.\n",
      "[INFO] 193 users scraped so far.\n",
      "[INFO] Scraping user: HilderscheidNuuday\n",
      "User match HilderscheidNuuday logged.\n",
      "[INFO] 194 users scraped so far.\n",
      "[INFO] Scraping user: niejo\n",
      "User match niejo logged.\n",
      "[INFO] 195 users scraped so far.\n",
      "[INFO] Scraping user: A60753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [34:22<22:19, 55.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company nuuday logged.\n",
      "[INFO] Scraping users for company: yousee\n",
      "[INFO] Scraping user: YouSee\n",
      "User match YouSee logged.\n",
      "[INFO] 196 users scraped so far.\n",
      "[INFO] Scraping user: youseedk\n",
      "User match youseedk logged.\n",
      "[INFO] 197 users scraped so far.\n",
      "[INFO] Scraping user: JakobGrosen\n",
      "User match JakobGrosen logged.\n",
      "[INFO] 198 users scraped so far.\n",
      "[INFO] Scraping user: YouSeeThisName\n",
      "User match YouSeeThisName logged.\n",
      "[INFO] 199 users scraped so far.\n",
      "[INFO] Scraping user: YouseeJenkinsCI\n",
      "User match YouseeJenkinsCI logged.\n",
      "[INFO] 200 users scraped so far.\n",
      "[INFO] Scraping user: anirbantdc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 41/64 [39:15<31:41, 82.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match anirbantdc logged.\n",
      "[INFO] 201 users scraped so far.\n",
      "[INFO] User niejo already scraped. Skipping.\n",
      "Company yousee logged.\n",
      "[INFO] Scraping users for company: relatel\n",
      "[INFO] Scraping user: relatel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 42/64 [42:08<34:56, 95.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match relatel logged.\n",
      "[INFO] 202 users scraped so far.\n",
      "Company relatel logged.\n",
      "[INFO] Scraping users for company: cphapp\n",
      "[INFO] Scraping user: cphapp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 43/64 [42:15<28:13, 80.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match cphapp logged.\n",
      "[INFO] 203 users scraped so far.\n",
      "Company cphapp logged.\n",
      "[INFO] Scraping users for company: commentor\n",
      "[INFO] Scraping user: commentorARM\n",
      "User match commentorARM logged.\n",
      "[INFO] 204 users scraped so far.\n",
      "[INFO] Scraping user: MichaelBoPoulsen\n",
      "User match MichaelBoPoulsen logged.\n",
      "[INFO] 205 users scraped so far.\n",
      "[INFO] Scraping user: PIHCommentor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 44/64 [42:25<22:21, 67.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match PIHCommentor logged.\n",
      "[INFO] 206 users scraped so far.\n",
      "Company commentor logged.\n",
      "[INFO] Scraping users for company: nabto\n",
      "[INFO] Scraping user: nabto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 45/64 [47:42<38:15, 120.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match nabto logged.\n",
      "[INFO] 207 users scraped so far.\n",
      "Company nabto logged.\n",
      "[INFO] Scraping users for company: jobindex\n",
      "[INFO] Scraping user: jobindex\n",
      "User match jobindex logged.\n",
      "[INFO] 208 users scraped so far.\n",
      "[INFO] Scraping user: Eckankar\n",
      "User match Eckankar logged.\n",
      "[INFO] 209 users scraped so far.\n",
      "[INFO] Scraping user: Jobindex-LH\n",
      "[INFO] Scraping user: Jobindexworld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 46/64 [54:27<56:16, 187.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company jobindex logged.\n",
      "[INFO] Scraping users for company: miracle\n",
      "[INFO] User miracle-as already scraped. Skipping.\n",
      "[INFO] User gahms already scraped. Skipping.\n",
      "[INFO] Scraping user: WSAudiology\n",
      "User match WSAudiology logged.\n",
      "[INFO] 210 users scraped so far.\n",
      "[INFO] Scraping user: Tahulrik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 47/64 [54:41<40:45, 143.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match Tahulrik logged.\n",
      "[INFO] 211 users scraped so far.\n",
      "Company miracle logged.\n",
      "[INFO] Scraping users for company: immeo\n",
      "[INFO] Scraping user: immeodk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 48/64 [54:48<28:43, 107.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match immeodk logged.\n",
      "[INFO] 212 users scraped so far.\n",
      "Company immeo logged.\n",
      "[INFO] Scraping users for company: siteimprove\n",
      "[INFO] Scraping user: Siteimprove\n",
      "User match Siteimprove logged.\n",
      "[INFO] 213 users scraped so far.\n",
      "[INFO] Scraping user: henrikklarup\n",
      "User match henrikklarup logged.\n",
      "[INFO] 214 users scraped so far.\n",
      "[INFO] Scraping user: mostergaard\n",
      "User match mostergaard logged.\n",
      "[INFO] 215 users scraped so far.\n",
      "[INFO] Scraping user: dcamsiteimprove\n",
      "User match dcamsiteimprove logged.\n",
      "[INFO] 216 users scraped so far.\n",
      "[INFO] Scraping user: martinatsiteimprove\n",
      "User match martinatsiteimprove logged.\n",
      "[INFO] 217 users scraped so far.\n",
      "[INFO] Scraping user: SorenHarderQESI\n",
      "User match SorenHarderQESI logged.\n",
      "[INFO] 218 users scraped so far.\n",
      "[INFO] Scraping user: platops-siteimprove\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 49/64 [1:01:03<45:16, 181.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match platops-siteimprove logged.\n",
      "[INFO] 219 users scraped so far.\n",
      "Company siteimprove logged.\n",
      "[INFO] Scraping users for company: cbrain\n",
      "[INFO] Scraping user: cBrain-dk\n",
      "User match cBrain-dk logged.\n",
      "[INFO] 220 users scraped so far.\n",
      "[INFO] Scraping user: cBrainAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [1:01:33<32:21, 138.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match cBrainAI logged.\n",
      "[INFO] 221 users scraped so far.\n",
      "Company cbrain logged.\n",
      "[INFO] Scraping users for company: deon digital\n",
      "[INFO] Scraping user: deondigital\n",
      "[INFO] Scraping user: DeonDigitalMarketingandWebAgency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 51/64 [1:01:36<21:35, 99.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company deon digital logged.\n",
      "[INFO] Scraping users for company: pwc\n",
      "[INFO] Scraping user: pwcdk-emil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 52/64 [1:01:39<14:20, 71.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match pwcdk-emil logged.\n",
      "[INFO] 222 users scraped so far.\n",
      "Company pwc logged.\n",
      "[INFO] Scraping users for company: studiesandme\n",
      "[INFO] Scraping user: StudiesAndMe\n",
      "[INFO] Scraping user: studiesandme-machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 53/64 [1:01:42<09:25, 51.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company studiesandme logged.\n",
      "[INFO] Scraping users for company: tv2\n",
      "[INFO] Scraping user: tv2\n",
      "User match tv2 logged.\n",
      "[INFO] 223 users scraped so far.\n",
      "[INFO] Scraping user: quartercastle\n",
      "Token cycled to ACCESS_TOKEN_2.\n",
      "Cycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /users/m110 failed with 403: Forbidden\n",
      "Setting next backoff to 1222.600527s\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    repo_limit=300\n",
    ")\n",
    "print(f'GitHub REST API ratelimit reset time for token {gs.current_token_key} is {gs.reset_time_point}. '\n",
    "      f'That will be in a little less than {gs.reset_time_in_minutes} minutes.')\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = 'first_tier_userinfo'\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for search_query, skip_location_filter, company_label in tqdm(company_location_filter_bool_zip, total=len(company_list_sample)):\n",
    "\n",
    "    # 3.1 Skip company if already scraped\n",
    "    if company_label in gs.companies_already_scraped:\n",
    "        print(f'[INFO] Company {company_label} already scraped. Skipping.')\n",
    "        continue\n",
    "\n",
    "    print(f'[INFO] Scraping users for company: {company_label}')\n",
    "    \n",
    "    # 3.2 Get users for this company\n",
    "    users = gs.get_gh_users(search_query, skip_location_filter)\n",
    "\n",
    "    # 3.3 Loop through users\n",
    "    for named_user, company in users:\n",
    "\n",
    "        # 3.3.1 Skip user if already scraped\n",
    "        if named_user.login in gs.users_already_scraped:\n",
    "            print(f'[INFO] User {named_user.login} already scraped. Skipping.')\n",
    "            continue\n",
    "\n",
    "        print(f'[INFO] Scraping user: {named_user.login}')\n",
    "        gs.users_already_scraped.add(named_user.login)\n",
    "\n",
    "        # 3.3.2 Get user info (may return None if repo limit exceeded or no match)\n",
    "        user_row = gs.get_user_info(named_user, company_label)\n",
    "        if user_row is None:\n",
    "            continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "        # 3.3.3 Extract match data\n",
    "        location_match = user_row.location\n",
    "        inferred_company = user_row.inferred_company\n",
    "        matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "        # 3.3.4 Save user info and log result\n",
    "        gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "        gs.log_user_w_match(named_user.login, inferred_company, matched_company_strings, location_match, user_log_path)\n",
    "        \n",
    "        print(f'[INFO] {gs.USERS_SCRAPED} users scraped so far.')\n",
    "\n",
    "    # 3.4 Log company after scraping all users\n",
    "    gs.log_company(company_label, company_log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
