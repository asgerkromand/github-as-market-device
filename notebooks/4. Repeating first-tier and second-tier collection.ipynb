{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e51a01",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c91c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc80c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub access token collected from config\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom functions\n",
    "from resources.github_functions import GithubScraper\n",
    "from resources.filter_functions import filter_ties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa9faa",
   "metadata": {},
   "source": [
    "## 0.1 File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928a4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "import resources.filepaths as fp\n",
    "\n",
    "fp_main = fp.fp_main\n",
    "fp_main_output = fp.fp_main_output\n",
    "\n",
    "# To output data that has to go to external s-drive\n",
    "fp_main_external = fp.fp_main_external\n",
    "fp_output_external = fp.fp_output_external"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132ead0",
   "metadata": {},
   "source": [
    "# 1. Load in the second tier company list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98214df7",
   "metadata": {},
   "source": [
    "## 1.2 Processing the company list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial list of companies\n",
    "second_tier_companies = pd.read_csv(fp_main_output / \"second_tier_companies.csv\")\n",
    "\n",
    "# Subset relevant company info for initial list\n",
    "list_of_company_info = second_tier_companies[\n",
    "    [\n",
    "        \"company_search_keyword\",\n",
    "        \"company_category\",\n",
    "        \"without_location_filter\",\n",
    "        \"company_label_name\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create company category map for initial list\n",
    "company_category_map = dict(\n",
    "    zip(\n",
    "        list_of_company_info[\"company_search_keyword\"],\n",
    "        list_of_company_info[\"company_category\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Output company category map\n",
    "with open(fp_main_output / \"company_category_map.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "    for keyword, category in company_category_map.items():\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                {\"company_search_keyword\": keyword, \"company_category\": category},\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip object to loop through, when querying companies.\n",
    "company_location_filter_bool_zip = zip(\n",
    "    list_of_company_info[\"company_search_keyword\"],\n",
    "    list_of_company_info[\"without_location_filter\"],\n",
    "    list_of_company_info[\"company_label_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d236999",
   "metadata": {},
   "source": [
    "# 2. Repeating the queriyng of second-tier company names and scraping users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b8970",
   "metadata": {},
   "source": [
    "## 2.1 Loading in scrapelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e440b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User log exists: first_tier_user_scrapelog.jsonl\n",
      "[INFO] Company log exists: company_scrapelog.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Collecting users already scraped\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "\n",
    "user_log_file_name = \"first_tier_user_scrapelog.jsonl\"\n",
    "company_log_file_name = \"company_scrapelog.jsonl\"\n",
    "user_log_path = fp_output_external / user_log_file_name\n",
    "company_log_path = fp_main_output / company_log_file_name\n",
    "\n",
    "# Ensure files exist and print message\n",
    "if user_log_path.exists():\n",
    "    print(f\"[INFO] User log exists: {user_log_path.name}\")\n",
    "else:\n",
    "    user_log_path.touch(exist_ok=True)\n",
    "\n",
    "if company_log_path.exists():\n",
    "    print(f\"[INFO] Company log exists: {company_log_path.name}\")\n",
    "else:\n",
    "    company_log_path.touch(exist_ok=True)\n",
    "\n",
    "# Read user log\n",
    "with open(user_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            user_info = json.loads(line)\n",
    "            users_already_scraped.add(user_info[\"user_login\"])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed user line: {err}\")\n",
    "\n",
    "# Read company log\n",
    "with open(company_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            company = json.loads(line)\n",
    "            companies_already_scraped.add(company[\"company_name\"])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed company line: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf059d",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a7fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 33 companies and 149 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token is 2025-08-13 17:11:54. That will be in a little less than 49 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scraping users for company: knowit\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: miracle-as\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: gahms\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    repo_limit=50,\n",
    ")\n",
    "print(\n",
    "    f\"GitHub REST API ratelimit reset time for token is {gs.reset_time_point}. \"\n",
    "    f\"That will be in a little less than {gs.reset_time_in_minutes} minutes.\"\n",
    ")\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = \"first_tier_userinfo\"\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for search_query, skip_location_filter, company_label in tqdm(\n",
    "    company_location_filter_bool_zip, total=len(second_tier_companies)\n",
    "):\n",
    "    # 3.1 Skip company if already scraped\n",
    "    if company_label in gs.companies_already_scraped:\n",
    "        print(f\"[INFO] Company {company_label} already scraped. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[INFO] Scraping users for company: {company_label}\")\n",
    "\n",
    "    # 3.2 Get users for this company\n",
    "    users = gs.get_gh_users(search_query, skip_location_filter)\n",
    "\n",
    "    # 3.3 Loop through users\n",
    "    for named_user, company in users:\n",
    "        # 3.3.1 Skip user if already scraped\n",
    "        if named_user.login in gs.users_already_scraped:\n",
    "            print(f\"[INFO] User {named_user.login} already scraped. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Scraping user: {named_user.login}\")\n",
    "        gs.users_already_scraped.add(named_user.login)\n",
    "\n",
    "        # 3.3.2 Get user info (may return None if repo limit exceeded or no match)\n",
    "        user_row = gs.get_user_info(named_user, company_label, company_filter=True)\n",
    "        if user_row is None:\n",
    "            continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "        # 3.3.3 Extract match data\n",
    "        location_match = user_row.matched_location\n",
    "        inferred_company = user_row.inferred_company\n",
    "        matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "        # 3.3.4 Save user info and log result\n",
    "        gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "        gs.log_user_w_match(\n",
    "            named_user.login,\n",
    "            inferred_company,\n",
    "            matched_company_strings,\n",
    "            location_match,\n",
    "            user_log_path,\n",
    "        )  # type: ignore\n",
    "\n",
    "        print(f\"[INFO] {gs.USERS_SCRAPED} users scraped so far.\")\n",
    "\n",
    "    # 3.4 Log company after scraping all users\n",
    "    gs.log_company(company_label, company_log_path)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31362c05",
   "metadata": {},
   "source": [
    "# 3 Adding new first-tier users to first-tier dataset\n",
    "\n",
    "*Note: Done by repeating notebook \"2. Creating first-tier dataset.ipynb\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee51b90",
   "metadata": {},
   "source": [
    "## 3.1 Opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db3f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file\n",
    "first_tier_file_name = \"first_tier_userinfo.jsonl\"\n",
    "fp_first_tier = fp_output_external / first_tier_file_name\n",
    "\n",
    "# Load the first tier data\n",
    "with open(fp_first_tier, \"r\") as f:\n",
    "    first_tier_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert to DataFrame\n",
    "first_tier_userinfo = pd.DataFrame(first_tier_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15ef95",
   "metadata": {},
   "source": [
    "## 3.2 Aggegating unique user connections for each first-tier user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_ties_columns = [\n",
    "    \"follows_in\",\n",
    "    \"follows_out\",\n",
    "    \"watches_in\",\n",
    "    \"watches_out\",\n",
    "    \"stars_in\",\n",
    "    \"stars_out\",\n",
    "    \"forks_in\",\n",
    "    \"forks_out\",\n",
    "]\n",
    "\n",
    "first_tier_userinfo[\"unique_ties\"] = first_tier_userinfo.apply(\n",
    "    lambda row: filter_ties(row, fetch_ties_columns), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70e5cb",
   "metadata": {},
   "source": [
    "## 3.3 Save the sorted DataFrame to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in dataset: 149\n"
     ]
    }
   ],
   "source": [
    "# Print number of users\n",
    "print(f\"Number of unique users in dataset: {len(first_tier_userinfo)}\")\n",
    "\n",
    "# Outputting sorted first-tier-user list with gzip (because of list within the dataframe)\n",
    "first_tier_userinfo.to_parquet(\n",
    "    fp_output_external / \"first_tier_ties_extended.parquet.gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a8a45",
   "metadata": {},
   "source": [
    "# 4 Repeating second-tier collection \n",
    "\n",
    "*Note: Done by repeating notebook \"2. Scraping second-tier users.ipynb\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8d6b9",
   "metadata": {},
   "source": [
    "## 4.1 Load in first-tier users extended version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tier_extended_info = pd.read_parquet(\n",
    "    fp_output_external / \"first_tier_ties_extended.parquet.gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081df258",
   "metadata": {},
   "source": [
    "## 4.2 Creating a dataframe where a row is a company with a list of potential second tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b4024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530\n"
     ]
    }
   ],
   "source": [
    "# Aggregate potential second users for each company in the second tier\n",
    "second_tier_extended_users_and_company = first_tier_extended_info.groupby(\n",
    "    \"search_with_company\", as_index=False\n",
    ")[\"unique_ties\"].agg(lambda x: list(chain.from_iterable(x)))\n",
    "\n",
    "# Calculate total number of potential second-tier users\n",
    "numb_of_second_tier_users = (\n",
    "    second_tier_extended_users_and_company[\"unique_ties\"].str.len().sum()\n",
    ")\n",
    "\n",
    "print(numb_of_second_tier_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fec58",
   "metadata": {},
   "source": [
    "## 4.3 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: second_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: users_attempted_scrape.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Initialize sets for tracking\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "users_attempted_scraped = set()\n",
    "\n",
    "# Paths\n",
    "first_tier_user_log_file = \"first_tier_userinfo_user_log.jsonl\"\n",
    "second_tier_user_log_file = \"second_tier_userinfo_user_log.jsonl\"\n",
    "users_attempted_scrape_file = \"users_attempted_scrape.jsonl\"\n",
    "\n",
    "first_tier_user_log_path = fp_output_external / first_tier_user_log_file\n",
    "second_tier_user_log_path = fp_output_external / second_tier_user_log_file\n",
    "users_attempted_scrape_path = fp_output_external / users_attempted_scrape_file\n",
    "\n",
    "\n",
    "def ensure_file_exists(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[INFO] File does not exist. Creating: {path.name}\")\n",
    "        path.touch(exist_ok=True)\n",
    "    else:\n",
    "        print(f\"[INFO] File exists: {path.name}\")\n",
    "\n",
    "\n",
    "def load_users_from_log(path: Path):\n",
    "    users = set()\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    user_info = json.loads(line)\n",
    "                    users.add(user_info[\"user_login\"])\n",
    "                except (json.JSONDecodeError, KeyError) as err:\n",
    "                    print(\n",
    "                        f\"[WARNING] Skipping malformed user line in {path.name}: {err}\"\n",
    "                    )\n",
    "    return users\n",
    "\n",
    "\n",
    "# Ensure all files exist\n",
    "for path in [\n",
    "    first_tier_user_log_path,\n",
    "    second_tier_user_log_path,\n",
    "    users_attempted_scrape_path,\n",
    "]:\n",
    "    ensure_file_exists(path)\n",
    "\n",
    "# Populate sets\n",
    "users_already_scraped |= load_users_from_log(first_tier_user_log_path)\n",
    "users_already_scraped |= load_users_from_log(second_tier_user_log_path)\n",
    "users_already_attempted = load_users_from_log(users_attempted_scrape_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 0 companies and 3 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token is 2025-08-12 14:39:58. That will be in a little less than 7 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/483 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User karuncs already scraped. Skipping.\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/483 [00:06<55:40,  6.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 3.5 Check if user is a relevant user (DK + company)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m user_row \u001b[38;5;241m=\u001b[39m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_user_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamed_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_with_company\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip user if they don't meet scraping criteria\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2. SODAS/99 Asger old notebooks/Attention network/GH scrape for real 3/resources/github_functions.py:122\u001b[0m, in \u001b[0;36mratelimiter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Reset time passed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(wait_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms ago, skipping sleep.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2. SODAS/99 Asger old notebooks/Attention network/GH scrape for real 3/resources/github_functions.py:701\u001b[0m, in \u001b[0;36mGithubScraper.get_user_info\u001b[0;34m(self, user, company_label, company_filter)\u001b[0m\n\u001b[1;32m    699\u001b[0m follows_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_follows_out(user)\n\u001b[1;32m    700\u001b[0m watches_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_watches_in(all_repos, user)\n\u001b[0;32m--> 701\u001b[0m watches_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_watches_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m stars_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stars_in(all_repos, user)\n\u001b[1;32m    703\u001b[0m stars_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stars_out(user)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2. SODAS/99 Asger old notebooks/Attention network/GH scrape for real 3/resources/github_functions.py:122\u001b[0m, in \u001b[0;36mratelimiter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Reset time passed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(wait_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms ago, skipping sleep.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2. SODAS/99 Asger old notebooks/Attention network/GH scrape for real 3/resources/github_functions.py:592\u001b[0m, in \u001b[0;36mGithubScraper.get_watches_out\u001b[0;34m(self, user)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mGet the users who are watching the specified repositories.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    List[Dict[str, str]]: A list of dictionaries containing watching information.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mowner_login\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreated_at\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_subscriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[get_watch_out] Failed for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;241m.\u001b[39mlogin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCopenhagen/2. SODAS/99 Asger old notebooks/Attention network/GH scrape for real 3/resources/github_functions.py:592\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mGet the users who are watching the specified repositories.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    List[Dict[str, str]]: A list of dictionaries containing watching information.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mowner_login\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreated_at\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_subscriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[get_watch_out] Failed for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;241m.\u001b[39mlogin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/PaginatedList.py:84\u001b[0m, in \u001b[0;36mPaginatedListBase.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__elements\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_couldGrow():\n\u001b[0;32m---> 84\u001b[0m     newElements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m newElements\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/PaginatedList.py:95\u001b[0m, in \u001b[0;36mPaginatedListBase._grow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_grow\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[0;32m---> 95\u001b[0m     newElements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchNextPage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__elements \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m newElements\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m newElements\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/PaginatedList.py:320\u001b[0m, in \u001b[0;36mPaginatedList._fetchNextPage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetchNextPage\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_rest:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# REST API pagination\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m         headers, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJsonAndCheck\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__nextUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__nextParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m         data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getPage(data, headers)\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/Requester.py:625\u001b[0m, in \u001b[0;36mRequester.requestJsonAndCheck\u001b[0;34m(self, verb, url, parameters, headers, input, follow_302_redirect)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequestJsonAndCheck\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     verb: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     follow_302_redirect: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m    615\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m    Send a request with JSON body.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__check(\n\u001b[0;32m--> 625\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJson\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__customConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfollow_302_redirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_302_redirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/Requester.py:959\u001b[0m, in \u001b[0;36mRequester.requestJson\u001b[0;34m(self, verb, url, parameters, headers, input, cnx, follow_302_redirect)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 959\u001b[0m status, responseHeaders, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requestEncode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_302_redirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_302_redirect\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m status, responseHeaders, output\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/Requester.py:1098\u001b[0m, in \u001b[0;36mRequester.__requestEncode\u001b[0;34m(self, cnx, verb, url, parameters, requestHeaders, input, encode, stream, follow_302_redirect)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     requestHeaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoded_input \u001b[38;5;241m=\u001b[39m encode(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNEW_DEBUG_FRAME(requestHeaders)\n\u001b[0;32m-> 1098\u001b[0m status, responseHeaders, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requestRaw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequestHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_302_redirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_302_redirect\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Consts\u001b[38;5;241m.\u001b[39mheaderRateRemaining \u001b[38;5;129;01min\u001b[39;00m responseHeaders \u001b[38;5;129;01mand\u001b[39;00m Consts\u001b[38;5;241m.\u001b[39mheaderRateLimit \u001b[38;5;129;01min\u001b[39;00m responseHeaders:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiting \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;66;03m# ints expected but sometimes floats returned: https://github.com/PyGithub/PyGithub/pull/2697\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(responseHeaders[Consts\u001b[38;5;241m.\u001b[39mheaderRateRemaining])),\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(responseHeaders[Consts\u001b[38;5;241m.\u001b[39mheaderRateLimit])),\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/Requester.py:1129\u001b[0m, in \u001b[0;36mRequester.__requestRaw\u001b[0;34m(self, cnx, verb, url, requestHeaders, input, stream, follow_302_redirect)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__requestRaw\u001b[39m(\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1121\u001b[0m     cnx: Optional[Union[HTTPRequestsConnectionClass, HTTPSRequestsConnectionClass]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     follow_302_redirect: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any], Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]:\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__deferRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         original_cnx \u001b[38;5;241m=\u001b[39m cnx\n",
      "File \u001b[0;32m~/.local/share/uv/venvs/github/lib/python3.11/site-packages/github/Requester.py:1215\u001b[0m, in \u001b[0;36mRequester.__deferRequest\u001b[0;34m(self, verb)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msleeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms before next GitHub request\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1215\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(defer)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    users_already_attempted=users_already_attempted,\n",
    "    repo_limit=50,\n",
    ")\n",
    "\n",
    "second_tier_users_to_scrape = {\n",
    "    user: row[\"search_with_company\"]\n",
    "    for _, row in second_tier_extended_users_and_company.iterrows()  # type: ignore\n",
    "    for user in row[\"unique_ties\"]\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"GitHub REST API ratelimit reset time for token is {gs.reset_time_point}. \"\n",
    "    f\"That will be in a little less than {gs.reset_time_in_minutes} minutes.\"\n",
    ")\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = \"second_tier_userinfo\"\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for user, search_with_company in tqdm(second_tier_users_to_scrape.items()):\n",
    "    # 3.3 Check if user is already scraped\n",
    "    if user in gs.users_already_attempted:\n",
    "        print(f\"[INFO] User {user} already scraped. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Log user to the set of already attempted users\n",
    "    gs.log_user_scrape_attempt(user, users_attempted_scrape_path)\n",
    "    gs.users_already_attempted.add(user)\n",
    "\n",
    "    # 3.1 Get user from the flattened dictionary\n",
    "    named_user = gs.get_user(user)\n",
    "\n",
    "    # 3.2 Check if user is None (e.g., if user is not found)\n",
    "    if named_user is None:\n",
    "        continue\n",
    "\n",
    "    # 3.5 Check if user is a relevant user (DK + company)\n",
    "    user_row = gs.get_user_info(named_user, search_with_company, company_filter=False)\n",
    "    if user_row is None:\n",
    "        continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "    # 3.3.3 Extract match data\n",
    "    location_match = user_row.matched_location\n",
    "    inferred_company = user_row.inferred_company\n",
    "    matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "    # 3.3.4 Save user info and log result\n",
    "    gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "    gs.log_user_w_match(\n",
    "        named_user.login,\n",
    "        inferred_company,\n",
    "        matched_company_strings,\n",
    "        location_match,\n",
    "        second_tier_user_log_path,\n",
    "    )  # type: ignore\n",
    "\n",
    "    print(f\"[INFO] {gs.USERS_SCRAPED} users scraped so far.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94599d28",
   "metadata": {},
   "source": [
    "## 4.4 Save the extended second-tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1090c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Load in second-tier users\n",
    "with open(fp_output_external / \"second_tier_userinfo.jsonl\", \"r\") as f:\n",
    "    second_tier_users = pd.DataFrame([json.loads(line) for line in f])\n",
    "\n",
    "# Print number of users\n",
    "print(f\"Number of unique users in dataset: {len(second_tier_users)}\")\n",
    "\n",
    "## Outputting sorted second-tier-user list with gzip (because of list within the dataframe)\n",
    "second_tier_users.to_parquet(\n",
    "    fp_output_external / \"second_tier_ties_extended.parquet.gzip\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
