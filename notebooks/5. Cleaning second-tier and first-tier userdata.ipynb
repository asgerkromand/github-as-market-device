{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "# NB. uncomment the line above first time you run this cell\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom functions\n",
    "import resources.filter_functions as filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "import resources.filepaths as fp\n",
    "\n",
    "fp_main = fp.fp_main\n",
    "fp_main_output = fp.fp_main_output\n",
    "\n",
    "# To output data that has to go to external s-drive\n",
    "fp_main_external = fp.fp_main_external\n",
    "fp_output_external = fp.fp_output_external"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loading in first and second tier data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the files\n",
    "first_tier_file_name = \"first_tier_ties_extended.parquet.gzip\"\n",
    "second_tier_file_name = \"second_tier_userinfo.jsonl\"\n",
    "fp_first_tier = fp_output_external / first_tier_file_name\n",
    "fp_second_tier = fp_output_external / second_tier_file_name\n",
    "\n",
    "# Load the first tier data\n",
    "first_tier_data_clean = pd.read_parquet(fp_first_tier)\n",
    "\n",
    "# Columns that should be lists\n",
    "columns_to_convert = [\"inferred_company\"]\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    if column in first_tier_data_clean.columns:\n",
    "        first_tier_data_clean[column] = first_tier_data_clean[column].apply(\n",
    "            lambda x: list(x) if isinstance(x, (list, np.ndarray)) else x\n",
    "        )\n",
    "\n",
    "# Load the second tier data\n",
    "with open(fp_second_tier, \"r\") as f:\n",
    "    second_tier_list = [json.loads(line) for line in f]\n",
    "\n",
    "# Type hint so Pylance can infer the type of the variable\n",
    "second_tier_data: pd.DataFrame = pd.DataFrame(second_tier_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filter second-tier users on company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "second_tier_users_clean = second_tier_data.copy()\n",
    "\n",
    "# Find matched company strings\n",
    "second_tier_users_clean[\"matched_company_strings\"] = second_tier_users_clean.apply(\n",
    "    lambda user_row: filter.search_for_company(\n",
    "        [\n",
    "            v\n",
    "            for v in [\n",
    "                user_row.get(\"user_login\"),\n",
    "                user_row.get(\"listed_company\"),\n",
    "                user_row.get(\"email\"),\n",
    "                user_row.get(\"bio\"),\n",
    "                user_row.get(\"blog\"),\n",
    "            ]\n",
    "            if v and str(v).strip()\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")  # type: ignore\n",
    "\n",
    "# Get the keys of the element in the dictionary of matched company strings\n",
    "second_tier_users_clean[\"inferred_company\"] = second_tier_users_clean[\n",
    "    \"matched_company_strings\"\n",
    "].apply(lambda x: list(x.keys()) if isinstance(x, dict) else [])\n",
    "\n",
    "# Filter on rows that have an inferred company\n",
    "second_tier_data_clean = second_tier_users_clean[\n",
    "    second_tier_users_clean[\"inferred_company\"].apply(len) > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merging first-tier and second-tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Merged user data shape: (202, 22)\n",
      "[INFO] Deduplicated user data shape: (194, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/l43jl9d15098xcnblnvk66jc0000gn/T/ipykernel_68873/3963180515.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  second_tier_data_clean['tier'] = 2\n"
     ]
    }
   ],
   "source": [
    "# Assign tier labels to each dataset\n",
    "first_tier_data_clean[\"tier\"] = 1\n",
    "second_tier_data_clean[\"tier\"] = 2\n",
    "\n",
    "# Combine both tiers into a single dataframe\n",
    "merged_users = pd.concat(\n",
    "    [first_tier_data_clean, second_tier_data_clean], ignore_index=True\n",
    ")\n",
    "print(f\"[INFO] Merged user data shape: {merged_users.shape}\")\n",
    "\n",
    "# Remove duplicate users based on GitHub login\n",
    "unique_users_data = merged_users.drop_duplicates(subset=\"user_login\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "print(f\"[INFO] Deduplicated user data shape: {unique_users_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resolve multiple matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[mbetrifork] has multiple company matches:\n",
      "Inferred Companies: ['trifork', 'must']\n",
      "\n",
      "Bio Information:\n",
      "  user_login: mbetrifork\n",
      "  search_with_company: must\n",
      "  usertype: User\n",
      "  listed_company: Trifork Public A/S\n",
      "  email: mbe@trifork.com\n",
      "  bio: Family, religion, friendship. These are the three demons you must slay if you wish to succeed in business.\n",
      "  blog: \n"
     ]
    }
   ],
   "source": [
    "# Access output path\n",
    "output_path = \"../output/resolved_multicompany_cases.jsonl\"\n",
    "\n",
    "# Resolve multiple companies\n",
    "unique_users_data_clean = filter.resolve_multiple_companies(\n",
    "    unique_users_data,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the filtered users\n",
    "unique_users_data_clean.to_parquet(fp_output_external / \"final_dataset.gzip.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
