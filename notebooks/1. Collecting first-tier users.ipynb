{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub access token collected from config: gith...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2\n",
    "\n",
    "# Python \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Custom Packages\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2150de2-7c1e-4645-9f29-eec02c6618a2",
   "metadata": {},
   "source": [
    "## 0.1 Set file paths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdded948-6a02-4ace-9948-0ebedf00120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "import resources.filepaths as fp\n",
    "\n",
    "fp_main = fp.fp_main\n",
    "fp_main_output = fp.fp_main_output\n",
    "\n",
    "# To output data that has to go to external s-drive\n",
    "fp_main_external = fp.fp_main_external\n",
    "fp_output_external = fp.fp_output_external"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in the initial company list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Processing the company list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial list of companies\n",
    "initial_list = pd.read_csv(\n",
    "    fp_main_output / \"initial_list.csv\"\n",
    ")\n",
    "\n",
    "# Subset relevant company info for initial list\n",
    "list_of_company_info = initial_list[\n",
    "    ['company_search_keyword', 'company_category', 'without_location_filter', 'company_label_name']\n",
    "]\n",
    "\n",
    "# Create company category map for initial list\n",
    "company_category_map = dict(zip(\n",
    "    list_of_company_info['company_search_keyword'],\n",
    "    list_of_company_info['company_category']\n",
    "))\n",
    "\n",
    "# Output company category map\n",
    "with open(fp_main_output / \"company_category_map.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for keyword, category in company_category_map.items():\n",
    "        f.write(json.dumps({'company_search_keyword': keyword, 'company_category': category}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip object to loop through, when querying companies.\n",
    "company_location_filter_bool_zip = zip(\n",
    "    list_of_company_info['company_search_keyword'],\n",
    "    list_of_company_info['without_location_filter'],\n",
    "    list_of_company_info['company_label_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Queriyng the company names and scraping users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in scrapelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User log exists: first_tier_user_scrapelog.jsonl\n",
      "[INFO] Company log exists: company_scrapelog.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Collecting users already scraped\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "\n",
    "user_log_file_name = 'first_tier_user_scrapelog.jsonl'\n",
    "company_log_file_name = 'company_scrapelog.jsonl'\n",
    "user_log_path = fp_output_external / user_log_file_name\n",
    "company_log_path = fp_main_output / company_log_file_name\n",
    "\n",
    "# Ensure files exist and print message\n",
    "if user_log_path.exists():\n",
    "    print(f\"[INFO] User log exists: {user_log_path.name}\")\n",
    "else:\n",
    "    user_log_path.touch(exist_ok=True)\n",
    "\n",
    "if company_log_path.exists():\n",
    "    print(f\"[INFO] Company log exists: {company_log_path.name}\")\n",
    "else:\n",
    "    company_log_path.touch(exist_ok=True)\n",
    "\n",
    "# Read user log\n",
    "with open(user_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            user_info = json.loads(line)\n",
    "            users_already_scraped.add(user_info['user_login'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed user line: {err}\")\n",
    "\n",
    "# Read company log\n",
    "with open(company_log_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            company = json.loads(line)\n",
    "            companies_already_scraped.add(company['company_name'])\n",
    "        except (json.JSONDecodeError, KeyError) as err:\n",
    "            print(f\"[WARNING] Skipping malformed company line: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 8 companies and 38 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token is 2025-08-12 15:41:28. That will be in a little less than 55 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Company nodes already scraped. Skipping.\n",
      "[INFO] Company abtion already scraped. Skipping.\n",
      "[INFO] Company heyday already scraped. Skipping.\n",
      "[INFO] Company trifork already scraped. Skipping.\n",
      "[INFO] Company frontit already scraped. Skipping.\n",
      "[INFO] Company holion already scraped. Skipping.\n",
      "[INFO] Company kruso already scraped. Skipping.\n",
      "[INFO] Company pandiweb already scraped. Skipping.\n",
      "[INFO] Scraping users for company: uptime\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] User andersmandersen already scraped. Skipping.\n",
      "[INFO] User uptime-development already scraped. Skipping.\n",
      "[INFO] Scraping user: uptimedk\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match uptimedk logged.\n",
      "[INFO] 39 users scraped so far.\n",
      "[INFO] Scraping user: alertdesk\n",
      "User match alertdesk logged.\n",
      "[INFO] 40 users scraped so far.\n",
      "[INFO] Scraping user: SubleGG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 9/33 [00:34<01:32,  3.87s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match SubleGG logged.\n",
      "[INFO] 41 users scraped so far.\n",
      "Company uptime logged.\n",
      "[INFO] Scraping users for company: charlie tango\n",
      "[INFO] Scraping user: thebuilder\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: charlie-tango\n",
      "User match charlie-tango logged.\n",
      "[INFO] 42 users scraped so far.\n",
      "[INFO] Scraping user: kristofferkjelde\n",
      "User match kristofferkjelde logged.\n",
      "[INFO] 43 users scraped so far.\n",
      "[INFO] Scraping user: mariatlund\n",
      "User match mariatlund logged.\n",
      "[INFO] 44 users scraped so far.\n",
      "[INFO] Scraping user: jacobcharlietango\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 10/33 [03:33<10:39, 27.79s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match jacobcharlietango logged.\n",
      "[INFO] 45 users scraped so far.\n",
      "Company charlie tango logged.\n",
      "[INFO] Scraping users for company: ffw\n",
      "[INFO] Scraping user: ffwagency\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 11/33 [05:16<14:42, 40.11s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match ffwagency logged.\n",
      "[INFO] 46 users scraped so far.\n",
      "Company ffw logged.\n",
      "[INFO] Scraping users for company: mysupport\n",
      "[INFO] Scraping user: MySupport-aps\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 12/33 [05:22<11:45, 33.61s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match MySupport-aps logged.\n",
      "[INFO] 47 users scraped so far.\n",
      "Company mysupport logged.\n",
      "[INFO] Scraping users for company: shape\n",
      "[INFO] Scraping user: shapehq\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: ShapeGames\n",
      "User match ShapeGames logged.\n",
      "[INFO] 48 users scraped so far.\n",
      "[INFO] Scraping user: 3shape\n",
      "User match 3shape logged.\n",
      "[INFO] 49 users scraped so far.\n",
      "[INFO] Scraping user: anderslime\n",
      "User match anderslime logged.\n",
      "[INFO] 50 users scraped so far.\n",
      "[INFO] Scraping user: dkcas11\n",
      "User match dkcas11 logged.\n",
      "[INFO] 51 users scraped so far.\n",
      "[INFO] Scraping user: sebastianlyserena\n",
      "User match sebastianlyserena logged.\n",
      "[INFO] 52 users scraped so far.\n",
      "[INFO] Scraping user: flexshape\n",
      "User match flexshape logged.\n",
      "[INFO] 53 users scraped so far.\n",
      "[INFO] Scraping user: kawaiipantsu\n",
      "User match kawaiipantsu logged.\n",
      "[INFO] 54 users scraped so far.\n",
      "[INFO] Scraping user: TusharRoy23\n",
      "[INFO] Scraping user: shape-interviews\n",
      "User match shape-interviews logged.\n",
      "[INFO] 55 users scraped so far.\n",
      "[INFO] Scraping user: eddzio\n",
      "User match eddzio logged.\n",
      "[INFO] 56 users scraped so far.\n",
      "[INFO] Scraping user: olivernybo\n",
      "User match olivernybo logged.\n",
      "[INFO] 57 users scraped so far.\n",
      "[INFO] Scraping user: AurelieGIRAUD\n",
      "User match AurelieGIRAUD logged.\n",
      "[INFO] 58 users scraped so far.\n",
      "[INFO] Scraping user: pabloocania\n",
      "User match pabloocania logged.\n",
      "[INFO] 59 users scraped so far.\n",
      "[INFO] Scraping user: cloandr\n",
      "[INFO] Scraping user: ShapeInterviews\n",
      "User match ShapeInterviews logged.\n",
      "[INFO] 60 users scraped so far.\n",
      "[INFO] Scraping user: shape-matheusfaleiro\n",
      "User match shape-matheusfaleiro logged.\n",
      "[INFO] 61 users scraped so far.\n",
      "[INFO] Scraping user: ShapeShiftersStudio\n",
      "User match ShapeShiftersStudio logged.\n",
      "[INFO] 62 users scraped so far.\n",
      "[INFO] Scraping user: Shape-Automation\n",
      "User match Shape-Automation logged.\n",
      "[INFO] 63 users scraped so far.\n",
      "[INFO] Scraping user: NovusInnovation\n",
      "[INFO] Scraping user: dolandinvest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 13/33 [17:16<59:32, 178.64s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match dolandinvest logged.\n",
      "[INFO] 64 users scraped so far.\n",
      "Company shape logged.\n",
      "[INFO] Scraping users for company: makeable\n",
      "[INFO] Scraping user: makeabledk\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 14/33 [17:17<43:26, 137.18s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "Company makeable logged.\n",
      "[INFO] Scraping users for company: mustache\n",
      "[INFO] Scraping user: mustachedk\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match mustachedk logged.\n",
      "[INFO] 65 users scraped so far.\n",
      "[INFO] Scraping user: mustachedkdev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 15/33 [19:14<39:36, 132.03s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match mustachedkdev logged.\n",
      "[INFO] 66 users scraped so far.\n",
      "Company mustache logged.\n",
      "[INFO] Scraping users for company: house of code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 16/33 [19:14<27:36, 97.41s/company] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company house of code logged.\n",
      "[INFO] Scraping users for company: greener pastures\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[INFO] Scraping user: Husemeyer\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: cspagnesi\n",
      "[INFO] Scraping user: Team-Greener-Pastures\n",
      "[INFO] Scraping user: cwru-greener-pastures\n",
      "[INFO] Scraping user: greener-pastures\n",
      "User match greener-pastures logged.\n",
      "[INFO] 67 users scraped so far.\n",
      "[INFO] Scraping user: GreenerPasturesConsulting\n",
      "[INFO] Scraping user: rs-sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 17/33 [19:29<19:56, 74.81s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company greener pastures logged.\n",
      "[INFO] Scraping users for company: axla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 18/33 [19:29<13:28, 53.87s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company axla logged.\n",
      "[INFO] Scraping users for company: snapp\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[INFO] Scraping user: snappdk\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 19/33 [19:44<09:56, 42.62s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match snappdk logged.\n",
      "[INFO] 68 users scraped so far.\n",
      "Company snapp logged.\n",
      "[INFO] Scraping users for company: appscaptain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 20/33 [19:44<06:35, 30.40s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company appscaptain logged.\n",
      "[INFO] Scraping users for company: adtomic\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 21/33 [19:45<04:19, 21.65s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company adtomic logged.\n",
      "[INFO] Scraping users for company: signifly\n",
      "[INFO] Scraping user: signifly\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: Zagoman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 22/33 [21:24<08:11, 44.65s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match Zagoman logged.\n",
      "[INFO] 69 users scraped so far.\n",
      "Company signifly logged.\n",
      "[INFO] Scraping users for company: creuna\n",
      "[INFO] Scraping user: arla-creuna\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 23/33 [21:28<05:23, 32.39s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match arla-creuna logged.\n",
      "[INFO] 70 users scraped so far.\n",
      "Company creuna logged.\n",
      "[INFO] Scraping users for company: strømlin\n",
      "[INFO] Scraping user: headnet\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 24/33 [21:29<03:27, 23.05s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "Company strømlin logged.\n",
      "[INFO] Scraping users for company: must\n",
      "[INFO] Scraping user: kennylevinsen\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[INFO] Scraping user: mustass\n",
      "[INFO] Scraping user: gugi9000\n",
      "User match gugi9000 logged.\n",
      "[INFO] 71 users scraped so far.\n",
      "[INFO] Scraping user: mustafatemiz\n",
      "[INFO] User mustachedk already scraped. Skipping.\n",
      "[INFO] Scraping user: MustafaSidiqi\n",
      "[INFO] Scraping user: morten-andersen\n",
      "User match morten-andersen logged.\n",
      "[INFO] 72 users scraped so far.\n",
      "[INFO] Scraping user: mustafalani\n",
      "[INFO] Scraping user: Musta-0\n",
      "[INFO] Scraping user: mustafauskuplu\n",
      "User match mustafauskuplu logged.\n",
      "[INFO] 73 users scraped so far.\n",
      "[INFO] Scraping user: sieTRIFORK\n",
      "User match sieTRIFORK logged.\n",
      "[INFO] 74 users scraped so far.\n",
      "[INFO] User mustachedkdev already scraped. Skipping.\n",
      "[INFO] Scraping user: mustiodk\n",
      "[INFO] Scraping user: musti1766\n",
      "[INFO] Scraping user: Mustafa-ALD\n",
      "[INFO] Scraping user: warbuck9\n",
      "User match warbuck9 logged.\n",
      "[INFO] 75 users scraped so far.\n",
      "[INFO] Scraping user: mbetrifork\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 25/33 [24:15<08:45, 65.74s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match mbetrifork logged.\n",
      "[INFO] 76 users scraped so far.\n",
      "Company must logged.\n",
      "[INFO] Scraping users for company: netcompany\n",
      "[INFO] Scraping user: netcompany\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match netcompany logged.\n",
      "[INFO] 77 users scraped so far.\n",
      "[INFO] Scraping user: AeroplaneMouse\n",
      "User match AeroplaneMouse logged.\n",
      "[INFO] 78 users scraped so far.\n",
      "[INFO] Scraping user: ninkaninus\n",
      "User match ninkaninus logged.\n",
      "[INFO] 79 users scraped so far.\n",
      "[INFO] Scraping user: ansolesen\n",
      "User match ansolesen logged.\n",
      "[INFO] 80 users scraped so far.\n",
      "[INFO] Scraping user: HusseinElZein\n",
      "User match HusseinElZein logged.\n",
      "[INFO] 81 users scraped so far.\n",
      "[INFO] Scraping user: barth010may\n",
      "User match barth010may logged.\n",
      "[INFO] 82 users scraped so far.\n",
      "[INFO] Scraping user: Rene4100\n",
      "User match Rene4100 logged.\n",
      "[INFO] 83 users scraped so far.\n",
      "[INFO] Scraping user: D-Kuzin\n",
      "User match D-Kuzin logged.\n",
      "[INFO] 84 users scraped so far.\n",
      "[INFO] Scraping user: Victorschimmell\n",
      "User match Victorschimmell logged.\n",
      "[INFO] 85 users scraped so far.\n",
      "[INFO] Scraping user: nc-nsre\n",
      "User match nc-nsre logged.\n",
      "[INFO] 86 users scraped so far.\n",
      "[INFO] Scraping user: mathiasoldfarm\n",
      "User match mathiasoldfarm logged.\n",
      "[INFO] 87 users scraped so far.\n",
      "[INFO] Scraping user: tobiasmaneschijn\n",
      "User match tobiasmaneschijn logged.\n",
      "[INFO] 88 users scraped so far.\n",
      "[INFO] Scraping user: Wisienkas\n",
      "User match Wisienkas logged.\n",
      "[INFO] 89 users scraped so far.\n",
      "[INFO] Scraping user: kjaerb\n",
      "User match kjaerb logged.\n",
      "[INFO] 90 users scraped so far.\n",
      "[INFO] Scraping user: BBPL\n",
      "User match BBPL logged.\n",
      "[INFO] 91 users scraped so far.\n",
      "[INFO] Scraping user: Cznielsen\n",
      "[WAIT] Remaining requests: 297. Sleeping for 1180.5s until 2025-08-12 15:42:58\n",
      "User match Cznielsen logged.\n",
      "[INFO] 92 users scraped so far.\n",
      "[INFO] Scraping user: marcusjoost\n",
      "User match marcusjoost logged.\n",
      "[INFO] 93 users scraped so far.\n",
      "[INFO] Scraping user: NCarmu\n",
      "User match NCarmu logged.\n",
      "[INFO] 94 users scraped so far.\n",
      "[INFO] Scraping user: victornie92\n",
      "User match victornie92 logged.\n",
      "[INFO] 95 users scraped so far.\n",
      "[INFO] Scraping user: CStage\n",
      "User match CStage logged.\n",
      "[INFO] 96 users scraped so far.\n",
      "[INFO] Scraping user: marcusTm\n",
      "User match marcusTm logged.\n",
      "[INFO] 97 users scraped so far.\n",
      "[INFO] Scraping user: JanettHolst290490\n",
      "User match JanettHolst290490 logged.\n",
      "[INFO] 98 users scraped so far.\n",
      "[INFO] Scraping user: TheRumle\n",
      "User match TheRumle logged.\n",
      "[INFO] 99 users scraped so far.\n",
      "[INFO] Scraping user: PolleAnker\n",
      "User match PolleAnker logged.\n",
      "[INFO] 100 users scraped so far.\n",
      "[INFO] Scraping user: nc-pelo\n",
      "User match nc-pelo logged.\n",
      "[INFO] 101 users scraped so far.\n",
      "[INFO] Scraping user: netcompany-oliverpetersen\n",
      "User match netcompany-oliverpetersen logged.\n",
      "[INFO] 102 users scraped so far.\n",
      "[INFO] Scraping user: nc-chra\n",
      "User match nc-chra logged.\n",
      "[INFO] 103 users scraped so far.\n",
      "[INFO] Scraping user: itsmebenpax\n",
      "User match itsmebenpax logged.\n",
      "[INFO] 104 users scraped so far.\n",
      "[INFO] Scraping user: Netcompany-SersanAslan\n",
      "User match Netcompany-SersanAslan logged.\n",
      "[INFO] 105 users scraped so far.\n",
      "[INFO] Scraping user: netcompany-sadiksahin\n",
      "User match netcompany-sadiksahin logged.\n",
      "[INFO] 106 users scraped so far.\n",
      "[INFO] Scraping user: NetcompanyMfs\n",
      "User match NetcompanyMfs logged.\n",
      "[INFO] 107 users scraped so far.\n",
      "[INFO] Scraping user: MartinSorensen\n",
      "User match MartinSorensen logged.\n",
      "[INFO] 108 users scraped so far.\n",
      "[INFO] Scraping user: nc-hela\n",
      "User match nc-hela logged.\n",
      "[INFO] 109 users scraped so far.\n",
      "[INFO] Scraping user: nc-anli\n",
      "User match nc-anli logged.\n",
      "[INFO] 110 users scraped so far.\n",
      "[INFO] Scraping user: nc-ssor\n",
      "User match nc-ssor logged.\n",
      "[INFO] 111 users scraped so far.\n",
      "[INFO] Scraping user: karlfargo\n",
      "User match karlfargo logged.\n",
      "[INFO] 112 users scraped so far.\n",
      "[INFO] Scraping user: nc-fal\n",
      "User match nc-fal logged.\n",
      "[INFO] 113 users scraped so far.\n",
      "[INFO] Scraping user: 3FCO\n",
      "User match 3FCO logged.\n",
      "[INFO] 114 users scraped so far.\n",
      "[INFO] Scraping user: nc-easj\n",
      "User match nc-easj logged.\n",
      "[INFO] 115 users scraped so far.\n",
      "[INFO] Scraping user: andreasvalentinpedersen\n",
      "User match andreasvalentinpedersen logged.\n",
      "[INFO] 116 users scraped so far.\n",
      "[INFO] Scraping user: LakaNC\n",
      "User match LakaNC logged.\n",
      "[INFO] 117 users scraped so far.\n",
      "[INFO] Scraping user: nc-sgc\n",
      "User match nc-sgc logged.\n",
      "[INFO] 118 users scraped so far.\n",
      "[INFO] Scraping user: olasnc\n",
      "User match olasnc logged.\n",
      "[INFO] 119 users scraped so far.\n",
      "[INFO] Scraping user: nc-hek\n",
      "User match nc-hek logged.\n",
      "[INFO] 120 users scraped so far.\n",
      "[INFO] Scraping user: nc-htf\n",
      "User match nc-htf logged.\n",
      "[INFO] 121 users scraped so far.\n",
      "[INFO] Scraping user: nc-imd\n",
      "User match nc-imd logged.\n",
      "[INFO] 122 users scraped so far.\n",
      "[INFO] Scraping user: kba-nc\n",
      "User match kba-nc logged.\n",
      "[INFO] 123 users scraped so far.\n",
      "[INFO] Scraping user: nc-dtan\n",
      "User match nc-dtan logged.\n",
      "[INFO] 124 users scraped so far.\n",
      "[INFO] Scraping user: NicolaiNC\n",
      "User match NicolaiNC logged.\n",
      "[INFO] 125 users scraped so far.\n",
      "[INFO] Scraping user: nc-mfp\n",
      "User match nc-mfp logged.\n",
      "[INFO] 126 users scraped so far.\n",
      "[INFO] Scraping user: nc-adrh\n",
      "User match nc-adrh logged.\n",
      "[INFO] 127 users scraped so far.\n",
      "[INFO] Scraping user: nc-jbm\n",
      "User match nc-jbm logged.\n",
      "[INFO] 128 users scraped so far.\n",
      "[INFO] Scraping user: nc-niar\n",
      "User match nc-niar logged.\n",
      "[INFO] 129 users scraped so far.\n",
      "[INFO] Scraping user: MIKKH\n",
      "User match MIKKH logged.\n",
      "[INFO] 130 users scraped so far.\n",
      "[INFO] Scraping user: nc-jcc\n",
      "User match nc-jcc logged.\n",
      "[INFO] 131 users scraped so far.\n",
      "[INFO] Scraping user: nc-mosm\n",
      "User match nc-mosm logged.\n",
      "[INFO] 132 users scraped so far.\n",
      "[INFO] Scraping user: UlmerDK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 26/33 [1:02:38<1:25:41, 734.43s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match UlmerDK logged.\n",
      "[INFO] 133 users scraped so far.\n",
      "Company netcompany logged.\n",
      "[INFO] Scraping users for company: systematic\n",
      "[INFO] Scraping user: jeme\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match jeme logged.\n",
      "[INFO] 134 users scraped so far.\n",
      "[INFO] Scraping user: michaelbui99\n",
      "User match michaelbui99 logged.\n",
      "[INFO] 135 users scraped so far.\n",
      "[INFO] Scraping user: JonasAxelsen\n",
      "User match JonasAxelsen logged.\n",
      "[INFO] 136 users scraped so far.\n",
      "[INFO] Scraping user: andreasmalling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/andreasmalling/iptv/subscribers failed with 403: Forbidden\n",
      "Request GET /repos/andreasmalling/iptv failed with 403: Forbidden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match andreasmalling logged.\n",
      "[INFO] 137 users scraped so far.\n",
      "[INFO] Scraping user: SkouRene\n",
      "User match SkouRene logged.\n",
      "[INFO] 138 users scraped so far.\n",
      "[INFO] Scraping user: EfrinGonzalez\n",
      "User match EfrinGonzalez logged.\n",
      "[INFO] 139 users scraped so far.\n",
      "[INFO] Scraping user: tobiasfrisenborg\n",
      "User match tobiasfrisenborg logged.\n",
      "[INFO] 140 users scraped so far.\n",
      "[INFO] Scraping user: SvenNielsen\n",
      "User match SvenNielsen logged.\n",
      "[INFO] 141 users scraped so far.\n",
      "[INFO] Scraping user: hasnaAlina\n",
      "User match hasnaAlina logged.\n",
      "[INFO] 142 users scraped so far.\n",
      "[INFO] Scraping user: JacobBangSSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 27/33 [1:10:26<1:05:27, 654.53s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match JacobBangSSE logged.\n",
      "[INFO] 143 users scraped so far.\n",
      "Company systematic logged.\n",
      "[INFO] Scraping users for company: capgemini\n",
      "[INFO] Scraping user: Capgemini-Denmark-I-D-MLOps\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match Capgemini-Denmark-I-D-MLOps logged.\n",
      "[INFO] 144 users scraped so far.\n",
      "[INFO] Scraping user: sravansamudrala\n",
      "User match sravansamudrala logged.\n",
      "[INFO] 145 users scraped so far.\n",
      "[INFO] Scraping user: saberesf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 28/33 [1:10:48<38:47, 465.42s/company]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match saberesf logged.\n",
      "[INFO] 146 users scraped so far.\n",
      "Company capgemini logged.\n",
      "[INFO] Scraping users for company: sas institute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 29/33 [1:10:49<21:44, 326.12s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company sas institute logged.\n",
      "[INFO] Scraping users for company: eg a/s\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n",
      "[INFO] Scraping user: EG-A-S\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "User match EG-A-S logged.\n",
      "[INFO] 147 users scraped so far.\n",
      "[INFO] Scraping user: EGByg\n",
      "User match EGByg logged.\n",
      "[INFO] 148 users scraped so far.\n",
      "[INFO] Scraping user: EG-A-S-TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 30/33 [1:15:01<15:12, 304.01s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match EG-A-S-TEST logged.\n",
      "[INFO] 149 users scraped so far.\n",
      "Company eg a/s logged.\n",
      "[INFO] Scraping users for company: kmd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 31/33 [1:15:02<07:06, 213.21s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company kmd logged.\n",
      "[INFO] Scraping users for company: adform\n",
      "[NEW] GitHub ratelimit threshold set to 5 (max rate: 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 32/33 [1:15:04<02:29, 149.62s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - no users found\n",
      "Company adform logged.\n",
      "[INFO] Scraping users for company: proactivedk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [1:15:05<00:00, 136.52s/company]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - no users found\n",
      "Company proactivedk logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    repo_limit=50\n",
    ")\n",
    "print(f'GitHub REST API ratelimit reset time for token is {gs.reset_time_point}. '\n",
    "      f'That will be in a little less than {gs.reset_time_in_minutes} minutes.')\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = 'first_tier_userinfo'\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for search_query, skip_location_filter, company_label in tqdm(company_location_filter_bool_zip, total=len(initial_list), unit='company'):\n",
    "\n",
    "    # 3.1 Skip company if already scraped\n",
    "    if company_label in gs.companies_already_scraped:\n",
    "        print(f'[INFO] Company {company_label} already scraped. Skipping.')\n",
    "        continue\n",
    "\n",
    "    print(f'[INFO] Scraping users for company: {company_label}')\n",
    "    \n",
    "    # 3.2 Get users for this company\n",
    "    users = gs.get_gh_users(search_query, skip_location_filter)\n",
    "\n",
    "    # 3.3 Loop through users\n",
    "    for named_user, company in users:\n",
    "\n",
    "        # 3.3.1 Skip user if already scraped\n",
    "        if named_user.login in gs.users_already_scraped:\n",
    "            print(f'[INFO] User {named_user.login} already scraped. Skipping.')\n",
    "            continue\n",
    "\n",
    "        print(f'[INFO] Scraping user: {named_user.login}')\n",
    "        gs.users_already_scraped.add(named_user.login)\n",
    "\n",
    "        # 3.3.2 Get user info (may return None if repo limit exceeded or no match)\n",
    "        user_row = gs.get_user_info(named_user, company_label, company_filter=True)\n",
    "        if user_row is None:\n",
    "            continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "        # 3.3.3 Extract match data\n",
    "        location_match = user_row.matched_location\n",
    "        inferred_company = user_row.inferred_company\n",
    "        matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "        # 3.3.4 Save user info and log result\n",
    "        gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "        gs.log_user_w_match(named_user.login, inferred_company, matched_company_strings, location_match, user_log_path) # type: ignore\n",
    "\n",
    "        print(f'[INFO] {gs.USERS_SCRAPED} users scraped so far.')\n",
    "\n",
    "    # 3.4 Log company after scraping all users\n",
    "    gs.log_company(company_label, company_log_path) # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
