{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub access token collected from config\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom functions\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "import resources.filepaths as fp\n",
    "\n",
    "fp_main = fp.fp_main\n",
    "fp_main_output = fp.fp_main_output\n",
    "\n",
    "# To output data that has to go to external s-drive\n",
    "fp_main_external = fp.fp_main_external\n",
    "fp_output_external = fp.fp_output_external"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Filtering users and making (named-user, company)-list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading in the data on first tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tier_info = pd.read_parquet(fp_output_external / \"first_tier_ties.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a dataframe where a row is a company with a list of potential second tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530\n"
     ]
    }
   ],
   "source": [
    "# Aggregate potential second users for each company in the second tier\n",
    "second_tier_users_and_company = first_tier_info.groupby(\n",
    "    \"search_with_company\", as_index=False\n",
    ")[\"unique_ties\"].agg(lambda x: list(chain.from_iterable(x)))\n",
    "\n",
    "# Calculate total number of potential second-tier users\n",
    "numb_of_second_tier_users = second_tier_users_and_company[\"unique_ties\"].str.len().sum()\n",
    "\n",
    "print(numb_of_second_tier_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in scrapelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: second_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: users_attempted_scrape.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Initialize sets for tracking\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "users_attempted_scraped = set()\n",
    "\n",
    "# Paths\n",
    "first_tier_user_log_file = \"first_tier_userinfo_user_log.jsonl\"\n",
    "second_tier_user_log_file = \"second_tier_userinfo_user_log.jsonl\"\n",
    "users_attempted_scrape_file = \"users_attempted_scrape.jsonl\"\n",
    "\n",
    "first_tier_user_log_path = fp_output_external / first_tier_user_log_file\n",
    "second_tier_user_log_path = fp_output_external / second_tier_user_log_file\n",
    "users_attempted_scrape_path = fp_output_external / users_attempted_scrape_file\n",
    "\n",
    "\n",
    "def ensure_file_exists(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[INFO] File does not exist. Creating: {path.name}\")\n",
    "        path.touch(exist_ok=True)\n",
    "    else:\n",
    "        print(f\"[INFO] File exists: {path.name}\")\n",
    "\n",
    "\n",
    "def load_users_from_log(path: Path):\n",
    "    users = set()\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    user_info = json.loads(line)\n",
    "                    users.add(user_info[\"user_login\"])\n",
    "                except (json.JSONDecodeError, KeyError) as err:\n",
    "                    print(\n",
    "                        f\"[WARNING] Skipping malformed user line in {path.name}: {err}\"\n",
    "                    )\n",
    "    return users\n",
    "\n",
    "\n",
    "# Ensure all files exist\n",
    "for path in [\n",
    "    first_tier_user_log_path,\n",
    "    second_tier_user_log_path,\n",
    "    users_attempted_scrape_path,\n",
    "]:\n",
    "    ensure_file_exists(path)\n",
    "\n",
    "# Populate sets\n",
    "users_already_scraped |= load_users_from_log(first_tier_user_log_path)\n",
    "users_already_scraped |= load_users_from_log(second_tier_user_log_path)\n",
    "users_already_attempted = load_users_from_log(users_attempted_scrape_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 0 companies and 105 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token is 2025-08-13 16:11:04. That will be in a little less than 5 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4132 [00:00<?, ?user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User hcarreras already scraped. Skipping.\n",
      "[INFO] User aboedker already scraped. Skipping.\n",
      "[INFO] User adminabtion already scraped. Skipping.\n",
      "[INFO] User martinvintherp already scraped. Skipping.\n",
      "[INFO] User GuldbekLEGO already scraped. Skipping.\n",
      "[INFO] User AngelleAbtion already scraped. Skipping.\n",
      "[INFO] User finnpedersenkazes already scraped. Skipping.\n",
      "[INFO] User karuncs already scraped. Skipping.\n",
      "[INFO] User eemailme already scraped. Skipping.\n",
      "[INFO] User Aberen already scraped. Skipping.\n",
      "[INFO] User reinisla already scraped. Skipping.\n",
      "[INFO] User AskeLange already scraped. Skipping.\n",
      "[INFO] User morgenhaar already scraped. Skipping.\n",
      "[INFO] User nauman already scraped. Skipping.\n",
      "[INFO] User djuric already scraped. Skipping.\n",
      "[INFO] User RobWu already scraped. Skipping.\n",
      "[INFO] User ozf already scraped. Skipping.\n",
      "[INFO] User MikkelHansenAbtion already scraped. Skipping.\n",
      "[INFO] User Kosai106 already scraped. Skipping.\n",
      "[INFO] User bohme already scraped. Skipping.\n",
      "[INFO] User Tejs-Abtion already scraped. Skipping.\n",
      "[INFO] User RassaLibre already scraped. Skipping.\n",
      "[INFO] User mulky-sulaiman already scraped. Skipping.\n",
      "[INFO] User vayurobins already scraped. Skipping.\n",
      "[INFO] User martincarlsen already scraped. Skipping.\n",
      "[INFO] User AlejandraValdivia already scraped. Skipping.\n",
      "[INFO] User JijoBose already scraped. Skipping.\n",
      "[INFO] User bokh already scraped. Skipping.\n",
      "[INFO] User simonask already scraped. Skipping.\n",
      "[INFO] User liljom already scraped. Skipping.\n",
      "[INFO] User andersklenke already scraped. Skipping.\n",
      "[INFO] User MadsZeneli already scraped. Skipping.\n",
      "[INFO] User Dynastig already scraped. Skipping.\n",
      "[INFO] User casiodk already scraped. Skipping.\n",
      "[INFO] User stefan-vrskovy already scraped. Skipping.\n",
      "[INFO] User heatherm already scraped. Skipping.\n",
      "[INFO] User paramadeep already scraped. Skipping.\n",
      "[INFO] User hrithikt already scraped. Skipping.\n",
      "[INFO] User Typeform already scraped. Skipping.\n",
      "[INFO] User viesii already scraped. Skipping.\n",
      "[INFO] User Stevemoretz already scraped. Skipping.\n",
      "[INFO] User sinisterchipmunk already scraped. Skipping.\n",
      "[INFO] User substancelab already scraped. Skipping.\n",
      "[INFO] User dj-abtion already scraped. Skipping.\n",
      "[INFO] User cbit-abtion already scraped. Skipping.\n",
      "[INFO] User pedryvo already scraped. Skipping.\n",
      "[INFO] User cosmic-banana already scraped. Skipping.\n",
      "[INFO] User adionditsak already scraped. Skipping.\n",
      "[INFO] User a-alak already scraped. Skipping.\n",
      "[INFO] User slamidtfyn already scraped. Skipping.\n",
      "[INFO] User iox already scraped. Skipping.\n",
      "[INFO] User thatdoogieguy already scraped. Skipping.\n",
      "[INFO] User allowishus-dev already scraped. Skipping.\n",
      "[INFO] User jekyll already scraped. Skipping.\n",
      "[INFO] User synth already scraped. Skipping.\n",
      "[INFO] User epistrephein already scraped. Skipping.\n",
      "[INFO] User TimMoser92 already scraped. Skipping.\n",
      "[INFO] User MartJohannsen already scraped. Skipping.\n",
      "[INFO] User parseb already scraped. Skipping.\n",
      "[INFO] User runephilosof-abtion already scraped. Skipping.\n",
      "[INFO] User namuit already scraped. Skipping.\n",
      "[INFO] User Guldbek already scraped. Skipping.\n",
      "[INFO] User GeoffAbtion already scraped. Skipping.\n",
      "[INFO] User minhng92 already scraped. Skipping.\n",
      "[INFO] User magnusfriis already scraped. Skipping.\n",
      "[INFO] User jeppester already scraped. Skipping.\n",
      "[INFO] User hellevibeke already scraped. Skipping.\n",
      "[INFO] User knoopx already scraped. Skipping.\n",
      "[INFO] User hacketyhack already scraped. Skipping.\n",
      "[INFO] User fraywing already scraped. Skipping.\n",
      "[INFO] User Citizen2028 already scraped. Skipping.\n",
      "[INFO] User angusshire already scraped. Skipping.\n",
      "[INFO] User IanAbildskou already scraped. Skipping.\n",
      "[INFO] User apneadiving already scraped. Skipping.\n",
      "[INFO] User albertoqa already scraped. Skipping.\n",
      "[INFO] User bureson already scraped. Skipping.\n",
      "[INFO] User aidanogues already scraped. Skipping.\n",
      "[INFO] User kaspernj already scraped. Skipping.\n",
      "[INFO] User afcapel already scraped. Skipping.\n",
      "[INFO] User alexrothenberg already scraped. Skipping.\n",
      "[INFO] User dcabo already scraped. Skipping.\n",
      "[INFO] User dhl already scraped. Skipping.\n",
      "[INFO] User markets already scraped. Skipping.\n",
      "[INFO] User josecolella already scraped. Skipping.\n",
      "[INFO] User PartidoDeInternet already scraped. Skipping.\n",
      "[INFO] User ppruiz already scraped. Skipping.\n",
      "[INFO] User clacloy already scraped. Skipping.\n",
      "[INFO] User juanruiztorres661 already scraped. Skipping.\n",
      "[INFO] User vemv already scraped. Skipping.\n",
      "[INFO] User PragTob already scraped. Skipping.\n",
      "[INFO] User Hobo already scraped. Skipping.\n",
      "[INFO] User RasmusEklund already scraped. Skipping.\n",
      "[INFO] User granadajs already scraped. Skipping.\n",
      "[INFO] User JJ already scraped. Skipping.\n",
      "[INFO] User abtion already scraped. Skipping.\n",
      "[INFO] User toblerone554 already scraped. Skipping.\n",
      "[INFO] User JannikStreek already scraped. Skipping.\n",
      "[INFO] User namankumarjangid already scraped. Skipping.\n",
      "[INFO] User tareksamni already scraped. Skipping.\n",
      "[INFO] User LoneKP already scraped. Skipping.\n",
      "[INFO] User Samu92 already scraped. Skipping.\n",
      "[INFO] User ello already scraped. Skipping.\n",
      "[INFO] User voodoorai2000 already scraped. Skipping.\n",
      "[INFO] User JAntonioMarin already scraped. Skipping.\n",
      "[INFO] User vercel already scraped. Skipping.\n",
      "[INFO] User rogercampos already scraped. Skipping.\n",
      "[INFO] User thoughtbot already scraped. Skipping.\n",
      "[INFO] User Emguma already scraped. Skipping.\n",
      "[INFO] User kostylo already scraped. Skipping.\n",
      "[INFO] User andris9 already scraped. Skipping.\n",
      "[INFO] User Armienn already scraped. Skipping.\n",
      "[INFO] User keja already scraped. Skipping.\n",
      "[INFO] User svenfuchs already scraped. Skipping.\n",
      "[INFO] User blhio already scraped. Skipping.\n",
      "[INFO] User n-singh already scraped. Skipping.\n",
      "[INFO] User jimacoff already scraped. Skipping.\n",
      "[INFO] User fabn already scraped. Skipping.\n",
      "[INFO] User streamio already scraped. Skipping.\n",
      "[INFO] User evisko already scraped. Skipping.\n",
      "[INFO] User bloodybit already scraped. Skipping.\n",
      "[INFO] User cvolund already scraped. Skipping.\n",
      "[INFO] User adrianadaria already scraped. Skipping.\n",
      "[INFO] User bjarkof already scraped. Skipping.\n",
      "[INFO] User balaboon already scraped. Skipping.\n",
      "[INFO] User Quarzi already scraped. Skipping.\n",
      "[INFO] User 10bestdesign already scraped. Skipping.\n",
      "[INFO] User bjslips already scraped. Skipping.\n",
      "[INFO] User bdobry already scraped. Skipping.\n",
      "[INFO] User itsecurityco already scraped. Skipping.\n",
      "[INFO] User neoneye already scraped. Skipping.\n",
      "[INFO] User standardrb already scraped. Skipping.\n",
      "[INFO] User galtzo-floss already scraped. Skipping.\n",
      "[INFO] User jreybert already scraped. Skipping.\n",
      "[INFO] User heroku already scraped. Skipping.\n",
      "[INFO] User roo-rb already scraped. Skipping.\n",
      "[INFO] User catsky already scraped. Skipping.\n",
      "[INFO] User munificent already scraped. Skipping.\n",
      "[INFO] User tridactyl already scraped. Skipping.\n",
      "[INFO] User voormedia already scraped. Skipping.\n",
      "[INFO] User tabler already scraped. Skipping.\n",
      "[INFO] User influitive already scraped. Skipping.\n",
      "[INFO] User tpope already scraped. Skipping.\n",
      "[INFO] User brooklynDev already scraped. Skipping.\n",
      "[INFO] User janko already scraped. Skipping.\n",
      "[INFO] User rubocop already scraped. Skipping.\n",
      "[INFO] User mainmatter already scraped. Skipping.\n",
      "[INFO] User lynndylanhurley already scraped. Skipping.\n",
      "[INFO] User ilyakatz already scraped. Skipping.\n",
      "[INFO] User Insti already scraped. Skipping.\n",
      "[INFO] User Dreamersoul already scraped. Skipping.\n",
      "[INFO] User zipmark already scraped. Skipping.\n",
      "[INFO] User bash-unit already scraped. Skipping.\n",
      "[INFO] User simon0191 already scraped. Skipping.\n",
      "[INFO] User iamthefox already scraped. Skipping.\n",
      "[INFO] User lostisland already scraped. Skipping.\n",
      "[INFO] User heartcombo already scraped. Skipping.\n",
      "[INFO] User mobajo already scraped. Skipping.\n",
      "[INFO] User arshwinth already scraped. Skipping.\n",
      "[INFO] User AI-Agent-Incubator-Month already scraped. Skipping.\n",
      "[INFO] User hallandsen already scraped. Skipping.\n",
      "[INFO] User nila1010 already scraped. Skipping.\n",
      "[INFO] User nelix already scraped. Skipping.\n",
      "[INFO] User Uvacoder already scraped. Skipping.\n",
      "[INFO] User romanIlyushin-bc already scraped. Skipping.\n",
      "[INFO] User andreychev already scraped. Skipping.\n",
      "[INFO] User chrisaspringer already scraped. Skipping.\n",
      "[INFO] User estherfinsen already scraped. Skipping.\n",
      "[INFO] User kristofferkjelde already scraped. Skipping.\n",
      "[INFO] User yenbekbay already scraped. Skipping.\n",
      "[INFO] User sasetkaKmd already scraped. Skipping.\n",
      "[INFO] User zag already scraped. Skipping.\n",
      "[INFO] User rcherny already scraped. Skipping.\n",
      "[INFO] User sebastiantecsi already scraped. Skipping.\n",
      "[INFO] User mhnnovicell already scraped. Skipping.\n",
      "[INFO] User nikitavoloboev already scraped. Skipping.\n",
      "[INFO] User dreijer-dev already scraped. Skipping.\n",
      "[INFO] User giovannipds already scraped. Skipping.\n",
      "[INFO] User FredeNy already scraped. Skipping.\n",
      "[INFO] User macbesu already scraped. Skipping.\n",
      "[INFO] User PatrissolJuns already scraped. Skipping.\n",
      "[INFO] User Steffi3rd already scraped. Skipping.\n",
      "[INFO] User Dtesch9 already scraped. Skipping.\n",
      "[INFO] User joehua87 already scraped. Skipping.\n",
      "[INFO] User deadcoder0904 already scraped. Skipping.\n",
      "[INFO] User caspardue already scraped. Skipping.\n",
      "[INFO] User Ilya-Meer already scraped. Skipping.\n",
      "[INFO] User aolde already scraped. Skipping.\n",
      "[INFO] User kskonecka already scraped. Skipping.\n",
      "[INFO] User namjul already scraped. Skipping.\n",
      "[INFO] User iamcsk already scraped. Skipping.\n",
      "[INFO] User leopinzon already scraped. Skipping.\n",
      "[INFO] User schnogz already scraped. Skipping.\n",
      "[INFO] User amorea9 already scraped. Skipping.\n",
      "[INFO] User shuding already scraped. Skipping.\n",
      "[INFO] User mfaridzia already scraped. Skipping.\n",
      "[INFO] User shunkakinoki already scraped. Skipping.\n",
      "[INFO] User seb-thomas already scraped. Skipping.\n",
      "[INFO] User kumomiX already scraped. Skipping.\n",
      "[INFO] User roybarber already scraped. Skipping.\n",
      "[INFO] User thdk already scraped. Skipping.\n",
      "[INFO] User nirtamir2 already scraped. Skipping.\n",
      "[INFO] User thebuilder already scraped. Skipping.\n",
      "[INFO] User langri-sha already scraped. Skipping.\n",
      "[INFO] User Absanater already scraped. Skipping.\n",
      "[INFO] User ReiSikk already scraped. Skipping.\n",
      "[INFO] User dohomi already scraped. Skipping.\n",
      "[INFO] User johot already scraped. Skipping.\n",
      "[INFO] User paralin already scraped. Skipping.\n",
      "[INFO] User StrangeFruit already scraped. Skipping.\n",
      "[INFO] User AppServiceProvider already scraped. Skipping.\n",
      "[INFO] User shaantanu9 already scraped. Skipping.\n",
      "[INFO] User hallojoe already scraped. Skipping.\n",
      "[INFO] User TobiasRoland123 already scraped. Skipping.\n",
      "[INFO] User gorilazish already scraped. Skipping.\n",
      "[INFO] User ivogt already scraped. Skipping.\n",
      "[INFO] User the-rob already scraped. Skipping.\n",
      "[INFO] User jide already scraped. Skipping.\n",
      "[INFO] User jimsheen already scraped. Skipping.\n",
      "[INFO] User Meandmybadself already scraped. Skipping.\n",
      "[INFO] User uniquelau already scraped. Skipping.\n",
      "[INFO] User steliyan already scraped. Skipping.\n",
      "[INFO] User Megoos already scraped. Skipping.\n",
      "[INFO] User Zneider already scraped. Skipping.\n",
      "[INFO] User jonasholbech already scraped. Skipping.\n",
      "[INFO] User mrmartineau already scraped. Skipping.\n",
      "[INFO] User ccheney already scraped. Skipping.\n",
      "[INFO] User maxdreamin already scraped. Skipping.\n",
      "[INFO] User iamsayan already scraped. Skipping.\n",
      "[INFO] User bitttttten already scraped. Skipping.\n",
      "[INFO] User SiwMHP already scraped. Skipping.\n",
      "[INFO] User NatC02 already scraped. Skipping.\n",
      "[INFO] User wonism already scraped. Skipping.\n",
      "[INFO] User sanjaykapilesh already scraped. Skipping.\n",
      "[INFO] User f3d0t already scraped. Skipping.\n",
      "[INFO] User jeremyraffin already scraped. Skipping.\n",
      "[INFO] User catalinadreisig already scraped. Skipping.\n",
      "[INFO] User fbosch already scraped. Skipping.\n",
      "[INFO] User sapegin already scraped. Skipping.\n",
      "[INFO] User einarlove already scraped. Skipping.\n",
      "[INFO] User mi2oon already scraped. Skipping.\n",
      "[INFO] User JanderSilv already scraped. Skipping.\n",
      "[INFO] User samarpanda already scraped. Skipping.\n",
      "[INFO] User chanand already scraped. Skipping.\n",
      "[INFO] User onthebusiness already scraped. Skipping.\n",
      "[INFO] User livzino already scraped. Skipping.\n",
      "[INFO] User ndamkjaer already scraped. Skipping.\n",
      "[INFO] User barbararcbf12 already scraped. Skipping.\n",
      "[INFO] User subhanbakh already scraped. Skipping.\n",
      "[INFO] User Sledzik already scraped. Skipping.\n",
      "[INFO] User maxwaiyaki already scraped. Skipping.\n",
      "[INFO] User finnmerlett already scraped. Skipping.\n",
      "[INFO] User fael already scraped. Skipping.\n",
      "[INFO] User y0n1 already scraped. Skipping.\n",
      "[INFO] User ArcherDs already scraped. Skipping.\n",
      "[INFO] User carlorizzante already scraped. Skipping.\n",
      "[INFO] User SlawekSkiba already scraped. Skipping.\n",
      "[INFO] User dimorphic already scraped. Skipping.\n",
      "[INFO] User ptruser already scraped. Skipping.\n",
      "[INFO] User leifoolsen already scraped. Skipping.\n",
      "[INFO] User czycha already scraped. Skipping.\n",
      "[INFO] User thepuskar already scraped. Skipping.\n",
      "[INFO] User TheFred94 already scraped. Skipping.\n",
      "[INFO] User tflx already scraped. Skipping.\n",
      "[INFO] User danielweck already scraped. Skipping.\n",
      "[INFO] User stub4ever already scraped. Skipping.\n",
      "[INFO] User habtamu already scraped. Skipping.\n",
      "[INFO] User krestineHaugaard already scraped. Skipping.\n",
      "[INFO] User jawang94 already scraped. Skipping.\n",
      "[INFO] User ryee-dev already scraped. Skipping.\n",
      "[INFO] User johnrackles already scraped. Skipping.\n",
      "[INFO] User christopherstyles already scraped. Skipping.\n",
      "[INFO] User illianyh already scraped. Skipping.\n",
      "[INFO] User pilotpirxie already scraped. Skipping.\n",
      "[INFO] User adnahmed already scraped. Skipping.\n",
      "[INFO] User zcreativelabs already scraped. Skipping.\n",
      "[INFO] User kettanaito already scraped. Skipping.\n",
      "[INFO] User kentcdodds already scraped. Skipping.\n",
      "[INFO] User Andjag already scraped. Skipping.\n",
      "[INFO] User fish-shell already scraped. Skipping.\n",
      "[INFO] User mswjs already scraped. Skipping.\n",
      "[INFO] User Sam821 already scraped. Skipping.\n",
      "[INFO] User crnacura already scraped. Skipping.\n",
      "[INFO] User node-formidable already scraped. Skipping.\n",
      "[INFO] User mathiasbynens already scraped. Skipping.\n",
      "[INFO] User sunesimonsen already scraped. Skipping.\n",
      "[INFO] User wolthers already scraped. Skipping.\n",
      "[INFO] User pmndrs already scraped. Skipping.\n",
      "[INFO] User jakearchibald already scraped. Skipping.\n",
      "[INFO] User jacobemcken already scraped. Skipping.\n",
      "[INFO] User jkcom already scraped. Skipping.\n",
      "[INFO] User perchjackson already scraped. Skipping.\n",
      "[INFO] User hakimel already scraped. Skipping.\n",
      "[INFO] User tailwindlabs already scraped. Skipping.\n",
      "[INFO] User microsoft already scraped. Skipping.\n",
      "[INFO] User styled-components already scraped. Skipping.\n",
      "[INFO] User addyosmani already scraped. Skipping.\n",
      "[INFO] User paulirish already scraped. Skipping.\n",
      "[INFO] User douglascrockford already scraped. Skipping.\n",
      "[INFO] User PaulSorensen already scraped. Skipping.\n",
      "[INFO] User TheLarkInn already scraped. Skipping.\n",
      "[INFO] User hanshvidberg already scraped. Skipping.\n",
      "[INFO] User nhk-i2m already scraped. Skipping.\n",
      "[INFO] User jryom already scraped. Skipping.\n",
      "[INFO] User TanStack already scraped. Skipping.\n",
      "[INFO] User wearebraid already scraped. Skipping.\n",
      "[INFO] User kluplau already scraped. Skipping.\n",
      "[INFO] User gaearon already scraped. Skipping.\n",
      "[INFO] User chriscoyier already scraped. Skipping.\n",
      "[INFO] User artisanofcode-archive already scraped. Skipping.\n",
      "[INFO] User vitejs already scraped. Skipping.\n",
      "[INFO] User sdras already scraped. Skipping.\n",
      "[INFO] User charlie-tango already scraped. Skipping.\n",
      "[INFO] User i18next already scraped. Skipping.\n",
      "[INFO] User seckinyasar already scraped. Skipping.\n",
      "[INFO] User flamingofar already scraped. Skipping.\n",
      "[INFO] User Caro8866 already scraped. Skipping.\n",
      "[INFO] User CarolineClo already scraped. Skipping.\n",
      "[INFO] User lacjkea already scraped. Skipping.\n",
      "[INFO] User TeaTreeDmitri already scraped. Skipping.\n",
      "[INFO] User rigretah already scraped. Skipping.\n",
      "[INFO] User giorgiameton already scraped. Skipping.\n",
      "[INFO] User jofhatkea already scraped. Skipping.\n",
      "[INFO] User Abdulhamidsa already scraped. Skipping.\n",
      "[INFO] User juliekolle already scraped. Skipping.\n",
      "[INFO] User Aryan-Mi already scraped. Skipping.\n",
      "[INFO] User frpaf already scraped. Skipping.\n",
      "[INFO] User robha7 already scraped. Skipping.\n",
      "[INFO] User ANovitsky already scraped. Skipping.\n",
      "[INFO] User Navami1997 already scraped. Skipping.\n",
      "[INFO] User hefjo already scraped. Skipping.\n",
      "[INFO] User andrzejthebestbeast already scraped. Skipping.\n",
      "[INFO] User JesperNoerregaard already scraped. Skipping.\n",
      "[INFO] User amruthashiv already scraped. Skipping.\n",
      "[INFO] User egdw-maufr already scraped. Skipping.\n",
      "[INFO] User niksberg already scraped. Skipping.\n",
      "[INFO] User ssark97 already scraped. Skipping.\n",
      "[INFO] User MetalFreak6666 already scraped. Skipping.\n",
      "[INFO] User FilipSorebrand already scraped. Skipping.\n",
      "[INFO] User Karthik-EGDK already scraped. Skipping.\n",
      "[INFO] User saleem-eg already scraped. Skipping.\n",
      "[INFO] User ananya-egdk already scraped. Skipping.\n",
      "[INFO] User TolgaAndicEG already scraped. Skipping.\n",
      "[INFO] User EG-Kimny already scraped. Skipping.\n",
      "[INFO] User PatrikGu already scraped. Skipping.\n",
      "[INFO] User nipoo8egdk already scraped. Skipping.\n",
      "[INFO] User Ashish93-mrx already scraped. Skipping.\n",
      "[INFO] User jbhatEG already scraped. Skipping.\n",
      "[INFO] User armic-eg already scraped. Skipping.\n",
      "[INFO] User VNAIK-EG already scraped. Skipping.\n",
      "[INFO] User PullMyPants already scraped. Skipping.\n",
      "[INFO] User SwapnilLomate already scraped. Skipping.\n",
      "[INFO] User egkenmo already scraped. Skipping.\n",
      "[INFO] User shrajan98 already scraped. Skipping.\n",
      "[INFO] User ChristianFlyC already scraped. Skipping.\n",
      "[INFO] User xxxak-egas already scraped. Skipping.\n",
      "[INFO] User hehro already scraped. Skipping.\n",
      "[INFO] User Uthde1 already scraped. Skipping.\n",
      "[INFO] User eg-skuma already scraped. Skipping.\n",
      "[INFO] User Dynaway-MadsHaslund already scraped. Skipping.\n",
      "[INFO] User abhiich already scraped. Skipping.\n",
      "[INFO] User Angara162 already scraped. Skipping.\n",
      "[INFO] User mfrigerio already scraped. Skipping.\n",
      "[INFO] User vinod087 already scraped. Skipping.\n",
      "[INFO] User laphieg already scraped. Skipping.\n",
      "[INFO] User JestinDevassyM already scraped. Skipping.\n",
      "[INFO] User bhkum already scraped. Skipping.\n",
      "[INFO] User izcie already scraped. Skipping.\n",
      "[INFO] User renethorsted already scraped. Skipping.\n",
      "[INFO] User NithinBG90 already scraped. Skipping.\n",
      "[INFO] User hansespenskorpen already scraped. Skipping.\n",
      "[INFO] User raval-eg already scraped. Skipping.\n",
      "[INFO] User shrinidhispooj already scraped. Skipping.\n",
      "[INFO] User akhilesh-shastry already scraped. Skipping.\n",
      "[INFO] User egdw-xxxyo already scraped. Skipping.\n",
      "[INFO] User rasmus-carlsson already scraped. Skipping.\n",
      "[INFO] User egdw-silek already scraped. Skipping.\n",
      "[INFO] User vudpo already scraped. Skipping.\n",
      "[INFO] User skara-eg already scraped. Skipping.\n",
      "[INFO] User Ramyashree-k8 already scraped. Skipping.\n",
      "[INFO] User szysl already scraped. Skipping.\n",
      "[INFO] User makrz-eg already scraped. Skipping.\n",
      "[INFO] User egdw-calma already scraped. Skipping.\n",
      "[INFO] User swnai already scraped. Skipping.\n",
      "[INFO] User sisin369 already scraped. Skipping.\n",
      "[INFO] User jobac-eg already scraped. Skipping.\n",
      "[INFO] User shosh-eg already scraped. Skipping.\n",
      "[INFO] User xxetr-eg already scraped. Skipping.\n",
      "[INFO] User junayedrahaman50 already scraped. Skipping.\n",
      "[INFO] User phkri already scraped. Skipping.\n",
      "[INFO] User egdw-rosak already scraped. Skipping.\n",
      "[INFO] User ATroita already scraped. Skipping.\n",
      "[INFO] User Suraj-Eg already scraped. Skipping.\n",
      "[INFO] User rijueg already scraped. Skipping.\n",
      "[INFO] User prithshenoy already scraped. Skipping.\n",
      "[INFO] User haskafs already scraped. Skipping.\n",
      "[INFO] User Krisser50 already scraped. Skipping.\n",
      "[INFO] User RanjithKB-EG already scraped. Skipping.\n",
      "[INFO] User anudeepsripathi already scraped. Skipping.\n",
      "[INFO] User vaukx-eg already scraped. Skipping.\n",
      "[INFO] User anjoe-eg already scraped. Skipping.\n",
      "[INFO] User benoo-eg already scraped. Skipping.\n",
      "[INFO] User jpalm-egas already scraped. Skipping.\n",
      "[INFO] User Bhaskarraam23 already scraped. Skipping.\n",
      "[INFO] User alieinikoveg already scraped. Skipping.\n",
      "[INFO] User karzy-eg already scraped. Skipping.\n",
      "[INFO] User jpablosierral already scraped. Skipping.\n",
      "[INFO] User vpooj777 already scraped. Skipping.\n",
      "[INFO] User anjha21 already scraped. Skipping.\n",
      "[INFO] User Manish-EG already scraped. Skipping.\n",
      "[INFO] User yakan-eg already scraped. Skipping.\n",
      "[INFO] User rabmu already scraped. Skipping.\n",
      "[INFO] User Maneesh870 already scraped. Skipping.\n",
      "[INFO] User srpai746 already scraped. Skipping.\n",
      "[INFO] User jayden-eg-dk already scraped. Skipping.\n",
      "[INFO] User sanai-eg already scraped. Skipping.\n",
      "[INFO] User luszy-eg already scraped. Skipping.\n",
      "[INFO] User Jayaramvalsakumar already scraped. Skipping.\n",
      "[INFO] User allealbaron already scraped. Skipping.\n",
      "[INFO] User MarekPolok already scraped. Skipping.\n",
      "[INFO] User RohanShetty22 already scraped. Skipping.\n",
      "[INFO] User isboxs already scraped. Skipping.\n",
      "[INFO] User egdw-hiban already scraped. Skipping.\n",
      "[INFO] User pryci-eg already scraped. Skipping.\n",
      "[INFO] User Harsharaj647 already scraped. Skipping.\n",
      "[INFO] User rajba11 already scraped. Skipping.\n",
      "[INFO] User Sandhya230419 already scraped. Skipping.\n",
      "[INFO] User svirkelyst already scraped. Skipping.\n",
      "[INFO] User navrapp already scraped. Skipping.\n",
      "[INFO] User egdw-tajel already scraped. Skipping.\n",
      "[INFO] User facebook already scraped. Skipping.\n",
      "[INFO] User arsuceno already scraped. Skipping.\n",
      "[INFO] User eriklindh already scraped. Skipping.\n",
      "[INFO] User Raxana-Prabha already scraped. Skipping.\n",
      "[INFO] User ManishhSalian already scraped. Skipping.\n",
      "[INFO] User purushotham-shetty already scraped. Skipping.\n",
      "[INFO] User xxiak-eg already scraped. Skipping.\n",
      "[INFO] User xxkup-eg already scraped. Skipping.\n",
      "[INFO] User UllasBannur-EG already scraped. Skipping.\n",
      "[INFO] User akash-eg already scraped. Skipping.\n",
      "[INFO] User gagsa-eg already scraped. Skipping.\n",
      "[INFO] User Daraj8861 already scraped. Skipping.\n",
      "[INFO] User asshe-eg already scraped. Skipping.\n",
      "[INFO] User tonayEG already scraped. Skipping.\n",
      "[INFO] User egdw-olesh already scraped. Skipping.\n",
      "[INFO] User ganha-eg already scraped. Skipping.\n",
      "[INFO] User Enegia already scraped. Skipping.\n",
      "[INFO] User rammo-eg already scraped. Skipping.\n",
      "[INFO] User debva-eg already scraped. Skipping.\n",
      "[INFO] User RamakanthShenoyM already scraped. Skipping.\n",
      "[INFO] User pejoh-eg already scraped. Skipping.\n",
      "[INFO] User xxstf-eg already scraped. Skipping.\n",
      "[INFO] User Gaurav-gapmx already scraped. Skipping.\n",
      "[INFO] User kavyakulal24 already scraped. Skipping.\n",
      "[INFO] User kumarvarun17 already scraped. Skipping.\n",
      "[INFO] User okrbr already scraped. Skipping.\n",
      "[INFO] User jehojn already scraped. Skipping.\n",
      "[INFO] User fozail-ops already scraped. Skipping.\n",
      "[INFO] User egdw-danip already scraped. Skipping.\n",
      "[INFO] User tomiuitto already scraped. Skipping.\n",
      "[INFO] User foull already scraped. Skipping.\n",
      "[INFO] User akhsa-eg already scraped. Skipping.\n",
      "[INFO] User anandfern already scraped. Skipping.\n",
      "[INFO] User alamakota222 already scraped. Skipping.\n",
      "[INFO] User bsing-eg already scraped. Skipping.\n",
      "[INFO] User Rosh611 already scraped. Skipping.\n",
      "[INFO] User thomasbjeldbak already scraped. Skipping.\n",
      "[INFO] User tibereg already scraped. Skipping.\n",
      "[INFO] User NikolajSloth already scraped. Skipping.\n",
      "[INFO] User thomas-paaske-hoejgaard already scraped. Skipping.\n",
      "[INFO] User ryjep already scraped. Skipping.\n",
      "[INFO] User Divsh99 already scraped. Skipping.\n",
      "[INFO] User xxiyk-eg already scraped. Skipping.\n",
      "[INFO] User egdw-supra already scraped. Skipping.\n",
      "[INFO] User StefanoMartin already scraped. Skipping.\n",
      "[INFO] User Rahul01shaik already scraped. Skipping.\n",
      "[INFO] User shudu-eg already scraped. Skipping.\n",
      "[INFO] User VineethMudoor already scraped. Skipping.\n",
      "[INFO] User JPFoxtrot already scraped. Skipping.\n",
      "[INFO] User xxejf already scraped. Skipping.\n",
      "[INFO] User JaneRasmussen already scraped. Skipping.\n",
      "[INFO] User prathapkr654 already scraped. Skipping.\n",
      "[INFO] User chiko-eg-dk already scraped. Skipping.\n",
      "[INFO] User egdw-xxpna already scraped. Skipping.\n",
      "[INFO] User srkup already scraped. Skipping.\n",
      "[INFO] User Nikhita1159 already scraped. Skipping.\n",
      "[INFO] User RakshithKateel already scraped. Skipping.\n",
      "[INFO] User xxpge already scraped. Skipping.\n",
      "[INFO] User dbergstroem already scraped. Skipping.\n",
      "[INFO] User trkas already scraped. Skipping.\n",
      "[INFO] User malja-eg already scraped. Skipping.\n",
      "[INFO] User jaison10 already scraped. Skipping.\n",
      "[INFO] User MykhailoEG already scraped. Skipping.\n",
      "[INFO] User nrech-EG already scraped. Skipping.\n",
      "[INFO] User RATMIDYNA already scraped. Skipping.\n",
      "[INFO] User mikpeegse already scraped. Skipping.\n",
      "[INFO] User Belchuke already scraped. Skipping.\n",
      "[INFO] User ThomasMorch already scraped. Skipping.\n",
      "[INFO] User gjrao-droid already scraped. Skipping.\n",
      "[INFO] User brbar-brianvinod already scraped. Skipping.\n",
      "[INFO] User klevinEG already scraped. Skipping.\n",
      "[INFO] User estromsnes already scraped. Skipping.\n",
      "[INFO] User kprav-eg already scraped. Skipping.\n",
      "[INFO] User xxntc-eg already scraped. Skipping.\n",
      "[INFO] User karolborows already scraped. Skipping.\n",
      "[INFO] User oanastoicescu already scraped. Skipping.\n",
      "[INFO] User safaban already scraped. Skipping.\n",
      "[INFO] User eg-danbe already scraped. Skipping.\n",
      "[INFO] User kultom86 already scraped. Skipping.\n",
      "[INFO] User eg-sidra already scraped. Skipping.\n",
      "[INFO] User snehalds already scraped. Skipping.\n",
      "[INFO] User prcho already scraped. Skipping.\n",
      "[INFO] User shawnjoywin already scraped. Skipping.\n",
      "[INFO] User Vaishak-p-shetty already scraped. Skipping.\n",
      "[INFO] User vigsh048 already scraped. Skipping.\n",
      "[INFO] User pkuma-eg already scraped. Skipping.\n",
      "[INFO] User EG-DESAM already scraped. Skipping.\n",
      "[INFO] User jamma-eg already scraped. Skipping.\n",
      "[INFO] User mertanEG already scraped. Skipping.\n",
      "[INFO] User Prach9980 already scraped. Skipping.\n",
      "[INFO] User AJayasabeen already scraped. Skipping.\n",
      "[INFO] User ShravanK-code already scraped. Skipping.\n",
      "[INFO] User sujan238 already scraped. Skipping.\n",
      "[INFO] User sajanb77 already scraped. Skipping.\n",
      "[INFO] User dabanegdk already scraped. Skipping.\n",
      "[INFO] User jamesmblair already scraped. Skipping.\n",
      "[INFO] User Krathish-sk already scraped. Skipping.\n",
      "[INFO] User JborrEG already scraped. Skipping.\n",
      "[INFO] User xxnxp already scraped. Skipping.\n",
      "[INFO] User MikeMarius already scraped. Skipping.\n",
      "[INFO] User Krathish-Karkala already scraped. Skipping.\n",
      "[INFO] User tshet-eg already scraped. Skipping.\n",
      "[INFO] User hineshateg already scraped. Skipping.\n",
      "[INFO] User egdw-elnai already scraped. Skipping.\n",
      "[INFO] User daesk already scraped. Skipping.\n",
      "[INFO] User Pratham-S-Shetty already scraped. Skipping.\n",
      "[INFO] User tazul-eg already scraped. Skipping.\n",
      "[INFO] User Nishma-EG already scraped. Skipping.\n",
      "[INFO] User steru-eg already scraped. Skipping.\n",
      "[INFO] User niyasnkoya already scraped. Skipping.\n",
      "[INFO] User tomaadland already scraped. Skipping.\n",
      "[INFO] User AndreasEdal already scraped. Skipping.\n",
      "[INFO] User nihso-EG already scraped. Skipping.\n",
      "[INFO] User kentaro-m already scraped. Skipping.\n",
      "[INFO] User egluszm already scraped. Skipping.\n",
      "[INFO] User Sumanth-EG22 already scraped. Skipping.\n",
      "[INFO] User lavimont already scraped. Skipping.\n",
      "[INFO] User Jayesh-Gowda07 already scraped. Skipping.\n",
      "[INFO] User sonaliegdk already scraped. Skipping.\n",
      "[INFO] User NiCrasta already scraped. Skipping.\n",
      "[INFO] User neerup already scraped. Skipping.\n",
      "[INFO] User lukszeg already scraped. Skipping.\n",
      "[INFO] User martinkloppjensen already scraped. Skipping.\n",
      "[INFO] User sandeepakn already scraped. Skipping.\n",
      "[INFO] User jakub-botwicz already scraped. Skipping.\n",
      "[INFO] User raaku-eg already scraped. Skipping.\n",
      "[INFO] User ArchakEg already scraped. Skipping.\n",
      "[INFO] User xxpta already scraped. Skipping.\n",
      "[INFO] User xxqwx-eg already scraped. Skipping.\n",
      "[INFO] User kshet98 already scraped. Skipping.\n",
      "[INFO] User microposmp already scraped. Skipping.\n",
      "[INFO] User patkr-eg already scraped. Skipping.\n",
      "[INFO] User sahab-eg already scraped. Skipping.\n",
      "[INFO] User Surentz already scraped. Skipping.\n",
      "[INFO] User sharatnaik442 already scraped. Skipping.\n",
      "[INFO] User narenkini already scraped. Skipping.\n",
      "[INFO] User VIKKUEG already scraped. Skipping.\n",
      "[INFO] User shashanka-somayaji already scraped. Skipping.\n",
      "[INFO] User keere27 already scraped. Skipping.\n",
      "[INFO] User CarinaAndersen already scraped. Skipping.\n",
      "[INFO] User Manjeeth-Shenoy already scraped. Skipping.\n",
      "[INFO] User xxkci already scraped. Skipping.\n",
      "[INFO] User xxhbmeg already scraped. Skipping.\n",
      "[INFO] User linnkrb already scraped. Skipping.\n",
      "[INFO] User kamathanjana already scraped. Skipping.\n",
      "[INFO] User Floyden-Monteiro already scraped. Skipping.\n",
      "[INFO] User vijloegdk already scraped. Skipping.\n",
      "[INFO] User sunileg already scraped. Skipping.\n",
      "[INFO] User egdw-xxvld already scraped. Skipping.\n",
      "[INFO] User darsz-eg already scraped. Skipping.\n",
      "[INFO] User romku-eg already scraped. Skipping.\n",
      "[INFO] User makon-dyn already scraped. Skipping.\n",
      "[INFO] User Harshithmm already scraped. Skipping.\n",
      "[INFO] User jokle-eg already scraped. Skipping.\n",
      "[INFO] User xxcvd-eg already scraped. Skipping.\n",
      "[INFO] User egdw-laase already scraped. Skipping.\n",
      "[INFO] User vijayalaxmi-vinku already scraped. Skipping.\n",
      "[INFO] User jotka already scraped. Skipping.\n",
      "[INFO] User xxkyseg already scraped. Skipping.\n",
      "[INFO] User nihal-eg already scraped. Skipping.\n",
      "[INFO] User pwojciechowski already scraped. Skipping.\n",
      "[INFO] User ronbrEG already scraped. Skipping.\n",
      "[INFO] User pohit-pooja already scraped. Skipping.\n",
      "[INFO] User suhshetty already scraped. Skipping.\n",
      "[INFO] User egdw-ischa already scraped. Skipping.\n",
      "[INFO] User anshul-eg already scraped. Skipping.\n",
      "[INFO] User sowmya0508 already scraped. Skipping.\n",
      "[INFO] User ewsyl-eg already scraped. Skipping.\n",
      "[INFO] User VidyashreeMAK already scraped. Skipping.\n",
      "[INFO] User dhaba-eg already scraped. Skipping.\n",
      "[INFO] User vimagegdk already scraped. Skipping.\n",
      "[INFO] User xxmsc-eg already scraped. Skipping.\n",
      "[INFO] User Rushalkraj already scraped. Skipping.\n",
      "[INFO] User SiergiejEG already scraped. Skipping.\n",
      "[INFO] User preetham-pai-d already scraped. Skipping.\n",
      "[INFO] User dabam-eg already scraped. Skipping.\n",
      "[INFO] User raopavan14 already scraped. Skipping.\n",
      "[INFO] User rux2025 already scraped. Skipping.\n",
      "[INFO] User esbenUnipluss already scraped. Skipping.\n",
      "[INFO] User egabjor already scraped. Skipping.\n",
      "[INFO] User KP-Pratham-S-Shetty already scraped. Skipping.\n",
      "[INFO] User MariuszNajda already scraped. Skipping.\n",
      "[INFO] User astau-eg already scraped. Skipping.\n",
      "[INFO] User okusk already scraped. Skipping.\n",
      "[INFO] User Anvitha2904 already scraped. Skipping.\n",
      "[INFO] User ChaithraB6 already scraped. Skipping.\n",
      "[INFO] User ESPD already scraped. Skipping.\n",
      "[INFO] User DanielTegelbergEG already scraped. Skipping.\n",
      "[INFO] User VaishnaviArebail already scraped. Skipping.\n",
      "[INFO] User vigkx already scraped. Skipping.\n",
      "[INFO] User yarnpkg already scraped. Skipping.\n",
      "[INFO] User Sachin-Hadimani already scraped. Skipping.\n",
      "[INFO] User hlobo99 already scraped. Skipping.\n",
      "[INFO] User dkuma-egdk already scraped. Skipping.\n",
      "[INFO] User AnushKatari-ankab already scraped. Skipping.\n",
      "[INFO] User debasishsahoo already scraped. Skipping.\n",
      "[INFO] User egRakeshC already scraped. Skipping.\n",
      "[INFO] User advsoEG already scraped. Skipping.\n",
      "[INFO] User shreyashetty03 already scraped. Skipping.\n",
      "[INFO] User sugow-eg already scraped. Skipping.\n",
      "[INFO] User harshitha-egdk already scraped. Skipping.\n",
      "[INFO] User radma-eg already scraped. Skipping.\n",
      "[INFO] User leasv-eg already scraped. Skipping.\n",
      "[INFO] User ChiragKottaryy already scraped. Skipping.\n",
      "[INFO] User NagendraOnEG already scraped. Skipping.\n",
      "[INFO] User laphiegd already scraped. Skipping.\n",
      "[INFO] User Yaskx already scraped. Skipping.\n",
      "[INFO] User egdw-xxmmi already scraped. Skipping.\n",
      "[INFO] User vjopi already scraped. Skipping.\n",
      "[INFO] User bmxptodk already scraped. Skipping.\n",
      "[INFO] User Sh4de already scraped. Skipping.\n",
      "[INFO] User mateusz-piasecki already scraped. Skipping.\n",
      "[INFO] User KEEGO-EG already scraped. Skipping.\n",
      "[INFO] User shalmalishiya already scraped. Skipping.\n",
      "[INFO] User jomam-eg already scraped. Skipping.\n",
      "[INFO] User prithvishenoyp already scraped. Skipping.\n",
      "[INFO] User Vindstrup already scraped. Skipping.\n",
      "[INFO] User madom-eg already scraped. Skipping.\n",
      "[INFO] User mikup24 already scraped. Skipping.\n",
      "[INFO] User karrihub already scraped. Skipping.\n",
      "[INFO] User nankueg already scraped. Skipping.\n",
      "[INFO] User anluk-eg already scraped. Skipping.\n",
      "[INFO] User bronio already scraped. Skipping.\n",
      "[INFO] User msohn001 already scraped. Skipping.\n",
      "[INFO] User karlkaal already scraped. Skipping.\n",
      "[INFO] User rajegdk already scraped. Skipping.\n",
      "[INFO] User abhiegdk already scraped. Skipping.\n",
      "[INFO] User SouraEGDK already scraped. Skipping.\n",
      "[INFO] User rujep already scraped. Skipping.\n",
      "[INFO] User RudraKumar24 already scraped. Skipping.\n",
      "[INFO] User mohvi-dev already scraped. Skipping.\n",
      "[INFO] User ramal-eg already scraped. Skipping.\n",
      "[INFO] User BartoszCzyBartlomiej already scraped. Skipping.\n",
      "[INFO] User NishithShibaroor9 already scraped. Skipping.\n",
      "[INFO] User mikkoaitta-aho already scraped. Skipping.\n",
      "[INFO] User srujan-02 already scraped. Skipping.\n",
      "[INFO] User kbjorklideg already scraped. Skipping.\n",
      "[INFO] User jajab-eg already scraped. Skipping.\n",
      "[INFO] User solvilindas already scraped. Skipping.\n",
      "[INFO] User durka1229 already scraped. Skipping.\n",
      "[INFO] User TimoOjala already scraped. Skipping.\n",
      "[INFO] User atoha-eg already scraped. Skipping.\n",
      "[INFO] User Assassyn already scraped. Skipping.\n",
      "[INFO] User Vigneshk5 already scraped. Skipping.\n",
      "[INFO] User egmosil already scraped. Skipping.\n",
      "[INFO] User ajacodk already scraped. Skipping.\n",
      "[INFO] User kpandCS already scraped. Skipping.\n",
      "[INFO] User sanjayshivaprabha already scraped. Skipping.\n",
      "[INFO] User Gauthamegdk already scraped. Skipping.\n",
      "[INFO] User MubarikAlhassan already scraped. Skipping.\n",
      "[INFO] User egdw-vcheb already scraped. Skipping.\n",
      "[INFO] User Apoorva-kotian already scraped. Skipping.\n",
      "[INFO] User umash22 already scraped. Skipping.\n",
      "[INFO] User Jyothi-R already scraped. Skipping.\n",
      "[INFO] User rshet-eg already scraped. Skipping.\n",
      "[INFO] User vaishnavtkm already scraped. Skipping.\n",
      "[INFO] User Valma-EG already scraped. Skipping.\n",
      "[INFO] User LinuxJS already scraped. Skipping.\n",
      "[INFO] User FatemaMohammad already scraped. Skipping.\n",
      "[INFO] User mazko-eg already scraped. Skipping.\n",
      "[INFO] User DynARashid already scraped. Skipping.\n",
      "[INFO] User gaohe-eg already scraped. Skipping.\n",
      "[INFO] User Gauthami-Jagadish already scraped. Skipping.\n",
      "[INFO] User prade18 already scraped. Skipping.\n",
      "[INFO] User xxfes-eg already scraped. Skipping.\n",
      "[INFO] User fnxptodk already scraped. Skipping.\n",
      "[INFO] User jabkou already scraped. Skipping.\n",
      "[INFO] User Sujeena22 already scraped. Skipping.\n",
      "[INFO] User Pujitha-R already scraped. Skipping.\n",
      "[INFO] User manow-eg already scraped. Skipping.\n",
      "[INFO] User emma8808 already scraped. Skipping.\n",
      "[INFO] User SwathikEG already scraped. Skipping.\n",
      "[INFO] User Harshitha-Nag already scraped. Skipping.\n",
      "[INFO] User Manoj-H-U already scraped. Skipping.\n",
      "[INFO] User egantho already scraped. Skipping.\n",
      "[INFO] User EGjerik already scraped. Skipping.\n",
      "[INFO] User DynaElcik already scraped. Skipping.\n",
      "[INFO] User GA-MA-KA already scraped. Skipping.\n",
      "[INFO] User xxyar-eg already scraped. Skipping.\n",
      "[INFO] User jeski-eg already scraped. Skipping.\n",
      "[INFO] User KNagendraKamath already scraped. Skipping.\n",
      "[INFO] User harga-eg already scraped. Skipping.\n",
      "[INFO] User raggiholm already scraped. Skipping.\n",
      "[INFO] User smish-11 already scraped. Skipping.\n",
      "[INFO] User shksx-eg already scraped. Skipping.\n",
      "[INFO] User jukkahyv already scraped. Skipping.\n",
      "[INFO] User jaypu-eg already scraped. Skipping.\n",
      "[INFO] User krzbr-eg already scraped. Skipping.\n",
      "[INFO] User ARTZAEG already scraped. Skipping.\n",
      "[INFO] User dedka-eg-dk already scraped. Skipping.\n",
      "[INFO] User xxcvl-eg already scraped. Skipping.\n",
      "[INFO] User egdw-cljmo already scraped. Skipping.\n",
      "[INFO] User xxqeq-eg already scraped. Skipping.\n",
      "[INFO] User egdw-patst already scraped. Skipping.\n",
      "[INFO] User Mohammediliyas766 already scraped. Skipping.\n",
      "[INFO] User petriteg already scraped. Skipping.\n",
      "[INFO] User gkath-eg already scraped. Skipping.\n",
      "[INFO] User andersmik35 already scraped. Skipping.\n",
      "[INFO] User ritik-eg already scraped. Skipping.\n",
      "[INFO] User YashWala already scraped. Skipping.\n",
      "[INFO] User LeneSmidt already scraped. Skipping.\n",
      "[INFO] User vidsoeg already scraped. Skipping.\n",
      "[INFO] User emiscEG already scraped. Skipping.\n",
      "[INFO] User xmilla99 already scraped. Skipping.\n",
      "[INFO] User AnkushshettyEG already scraped. Skipping.\n",
      "[INFO] User xxhie-eg already scraped. Skipping.\n",
      "[INFO] User lakask already scraped. Skipping.\n",
      "[INFO] User TerjeHolte already scraped. Skipping.\n",
      "[INFO] User jungloev already scraped. Skipping.\n",
      "[INFO] User valthebald already scraped. Skipping.\n",
      "[INFO] User alexanderkotev already scraped. Skipping.\n",
      "[INFO] User spassimeonov already scraped. Skipping.\n",
      "[INFO] User undertext already scraped. Skipping.\n",
      "[INFO] User thanhdo1991 already scraped. Skipping.\n",
      "[INFO] User donjordano already scraped. Skipping.\n",
      "[INFO] User phannguyen already scraped. Skipping.\n",
      "[INFO] User mohammadanwar already scraped. Skipping.\n",
      "[INFO] User cwcarlsen already scraped. Skipping.\n",
      "[INFO] User vuchkov already scraped. Skipping.\n",
      "[INFO] User taracila already scraped. Skipping.\n",
      "[INFO] User dydimitrov already scraped. Skipping.\n",
      "[INFO] User dcos already scraped. Skipping.\n",
      "[INFO] User GabrielaTodorova already scraped. Skipping.\n",
      "[INFO] User konfuzed already scraped. Skipping.\n",
      "[INFO] User nobledm already scraped. Skipping.\n",
      "[INFO] User ryanowich already scraped. Skipping.\n",
      "[INFO] User lslinnet already scraped. Skipping.\n",
      "[INFO] User crystalstorm already scraped. Skipping.\n",
      "[INFO] User ZIvanova already scraped. Skipping.\n",
      "[INFO] User davidjguru already scraped. Skipping.\n",
      "[INFO] User ffw-hai-cao already scraped. Skipping.\n",
      "[INFO] User rradoychev already scraped. Skipping.\n",
      "[INFO] User lachezarvalchev already scraped. Skipping.\n",
      "[INFO] User svetlin-marinov already scraped. Skipping.\n",
      "[INFO] User RumenDamyanov already scraped. Skipping.\n",
      "[INFO] User OS2web already scraped. Skipping.\n",
      "[INFO] User HeoChauA already scraped. Skipping.\n",
      "[INFO] User rastepanyan already scraped. Skipping.\n",
      "[INFO] User beltofte already scraped. Skipping.\n",
      "[INFO] User alex-propeople already scraped. Skipping.\n",
      "[INFO] User tassoskoutlas already scraped. Skipping.\n",
      "[INFO] User skifter already scraped. Skipping.\n",
      "[INFO] User kasper-krogh-ffw already scraped. Skipping.\n",
      "[INFO] User knibals already scraped. Skipping.\n",
      "[INFO] User rosiivanova already scraped. Skipping.\n",
      "[INFO] User khristovffw already scraped. Skipping.\n",
      "[INFO] User mattpaz already scraped. Skipping.\n",
      "[INFO] User ppetkov89 already scraped. Skipping.\n",
      "[INFO] User stankut already scraped. Skipping.\n",
      "[INFO] User febdao already scraped. Skipping.\n",
      "[INFO] User bellcom already scraped. Skipping.\n",
      "[INFO] User n1kxx already scraped. Skipping.\n",
      "[INFO] User darkosubic already scraped. Skipping.\n",
      "[INFO] User capuleto already scraped. Skipping.\n",
      "[INFO] User Neda-Veleva already scraped. Skipping.\n",
      "[INFO] User mortenc already scraped. Skipping.\n",
      "[INFO] User acquia already scraped. Skipping.\n",
      "[INFO] User ArtemT-FFW already scraped. Skipping.\n",
      "[INFO] User jakobdo already scraped. Skipping.\n",
      "[INFO] User Lidastein already scraped. Skipping.\n",
      "[INFO] User aldunchev already scraped. Skipping.\n",
      "[INFO] User RumenYordanov already scraped. Skipping.\n",
      "[INFO] User MichaelThmsn already scraped. Skipping.\n",
      "[INFO] User carsten-propeople already scraped. Skipping.\n",
      "[INFO] User ntbpy already scraped. Skipping.\n",
      "[INFO] User rafistepanyan already scraped. Skipping.\n",
      "[INFO] User welin already scraped. Skipping.\n",
      "[INFO] User Nauticus already scraped. Skipping.\n",
      "[INFO] User RosenPetrovFFW already scraped. Skipping.\n",
      "[INFO] User mihaimoscovici already scraped. Skipping.\n",
      "[INFO] User rycbGithub already scraped. Skipping.\n",
      "[INFO] User NexagonIVS already scraped. Skipping.\n",
      "[INFO] User SimonVT already scraped. Skipping.\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n",
      "[WAIT] Remaining requests: 0. Sleeping for 348.5s until 2025-08-13 16:12:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 804/4132 [06:10<26:05,  2.13user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match mnexo logged.\n",
      "[INFO] 106 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 805/4132 [08:16<43:40,  1.27user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match jenswilly logged.\n",
      "[INFO] 107 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 808/4132 [08:34<50:09,  1.10user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match nielslbeck logged.\n",
      "[INFO] 108 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 838/4132 [09:54<11:36:30, 12.69s/user]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match nielsdaw logged.\n",
      "[INFO] 109 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 846/4132 [22:57<214:12:02, 234.67s/user]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match ecederstrand logged.\n",
      "[INFO] 110 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 854/4132 [23:53<26:42:42, 29.34s/user]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User match rkk logged.\n",
      "[INFO] 111 users scraped so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 874/4132 [24:15<59:46,  1.10s/user]   Request GET /users/HeroMeiKong failed with 403: Forbidden\n",
      "Setting next backoff to 1036.793931s\n"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    users_already_attempted=users_already_attempted,\n",
    "    repo_limit=50,\n",
    ")\n",
    "\n",
    "second_tier_users_to_scrape = {\n",
    "    user: row[\"search_with_company\"]\n",
    "    for _, row in second_tier_users_and_company.iterrows()  # type: ignore\n",
    "    for user in row[\"unique_ties\"]\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"GitHub REST API ratelimit reset time for token is {gs.reset_time_point}. \"\n",
    "    f\"That will be in a little less than {gs.reset_time_in_minutes} minutes.\"\n",
    ")\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = \"second_tier_userinfo\"\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for user, search_with_company in tqdm(second_tier_users_to_scrape.items(), unit=\"user\"):\n",
    "    # 3.3 Check if user is already scraped\n",
    "    if user in gs.users_already_attempted:\n",
    "        print(f\"[INFO] User {user} already scraped. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Log user to the set of already attempted users\n",
    "    gs.log_user_scrape_attempt(user, users_attempted_scrape_path)\n",
    "    gs.users_already_attempted.add(user)\n",
    "\n",
    "    # 3.1 Get user from the flattened dictionary\n",
    "    named_user = gs.get_user(user)\n",
    "\n",
    "    # 3.2 Check if user is None (e.g., if user is not found)\n",
    "    if named_user is None:\n",
    "        continue\n",
    "\n",
    "    # 3.5 Check if user is a relevant user (DK + company)\n",
    "    user_row = gs.get_user_info(named_user, search_with_company, company_filter=False)\n",
    "    if user_row is None:\n",
    "        continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "    # 3.3.3 Extract match data\n",
    "    location_match = user_row.matched_location\n",
    "    inferred_company = user_row.inferred_company\n",
    "    matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "    # 3.3.4 Save user info and log result\n",
    "    gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "    gs.log_user_w_match(\n",
    "        named_user.login,\n",
    "        inferred_company,\n",
    "        matched_company_strings,\n",
    "        location_match,\n",
    "        second_tier_user_log_path,\n",
    "    )  # type: ignore\n",
    "\n",
    "    print(f\"[INFO] {gs.USERS_SCRAPED} users scraped so far.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Scouting New Danish App Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Saving the second tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of second-tier users filtered: 3\n"
     ]
    }
   ],
   "source": [
    "# Load in second-tier users\n",
    "with open(fp_output_external / \"second_tier_userinfo.jsonl\", \"r\") as f:\n",
    "    second_tier_users = [json.loads(line) for line in f]\n",
    "\n",
    "# Print number of users\n",
    "print(f\"Number of unique users in dataset: {len(second_tier_users)}\")\n",
    "\n",
    "# Outputting sorted second-tier-user list with gzip (because of list within the dataframe)\n",
    "second_tier_users.to_parquet(fp_output_external / \"second_tier_ties.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Make a filtered copy of second-tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy to subset\n",
    "second_tier_users_subset = second_tier_users.copy()\n",
    "\n",
    "# Making a company mask, indicating whether there is an entry or not in the GitHub location variable\n",
    "company_mask = [bool(user.get(\"listed_company\")) for user in second_tier_users_subset]\n",
    "\n",
    "# Subset the second tier users based on the company mask\n",
    "second_tier_users_filtered = [\n",
    "    user for user, keep in zip(second_tier_users_subset, company_mask) if keep\n",
    "]\n",
    "\n",
    "print(f\"Number of second-tier users filtered: {len(second_tier_users_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Output for manual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE ON MANUAL INSPECTION**\n",
    "\n",
    "The outputted file \"second_tier_users_filtered_subset.csv\" will be manually gone through in a csv-reader program. From this a list of second-tier companies, which we choose to include in our sample, will be produced and found in the file: \".resources/second_tier_companies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset on bio variables to output for manual inspection of new companies\n",
    "variables = [\n",
    "    \"user_login\",\n",
    "    \"listed_company\",\n",
    "    \"inferred_company\",\n",
    "    \"email\",\n",
    "    \"bio\",\n",
    "    \"blog\",\n",
    "    \"github_location\",\n",
    "    \"matched_location\",\n",
    "]\n",
    "\n",
    "# Create a filtered dataset for manual inspection\n",
    "second_tier_users_filtered_subset = [\n",
    "    {var: user.get(var) for var in variables} for user in second_tier_users_filtered\n",
    "]\n",
    "\n",
    "# Create empty column to input new companies.\n",
    "for user in second_tier_users_filtered_subset:\n",
    "    user[\"new_company\"] = None\n",
    "\n",
    "# Output the subset for review as csv\n",
    "df = pd.DataFrame(second_tier_users_filtered_subset)\n",
    "df.to_csv(fp_output_external / \"second_tier_users_filtered_subset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
