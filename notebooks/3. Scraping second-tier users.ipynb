{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub access token collected from config: gith...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load the autoreload extension to automatically reload modules before executing code (to avoid restarting the kernel)\n",
    "%load_ext autoreload \n",
    "\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9367c19-1737-4928-ad62-f530f4bbd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom functions\n",
    "from resources.github_functions import GithubScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "import resources.filepaths as fp\n",
    "\n",
    "fp_main = fp.fp_main\n",
    "fp_main_output = fp.fp_main_output\n",
    "\n",
    "# To output data that has to go to external s-drive\n",
    "fp_main_external = fp.fp_main_external\n",
    "fp_output_external = fp.fp_output_external"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Filtering users and making (named-user, company)-list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading in the data on first tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tier_info = pd.read_parquet(fp_output_external / 'first_tier_ties.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a dataframe where a row is a company with a list of potential second tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530\n"
     ]
    }
   ],
   "source": [
    "# Aggregate potential second users for each company in the second tier\n",
    "second_tier_users_and_company = (\n",
    "    first_tier_info.groupby('search_with_company', as_index=False)['unique_ties']\n",
    "    .agg(lambda x: list(chain.from_iterable(x)))\n",
    ")\n",
    "\n",
    "# Calculate total number of potential second-tier users\n",
    "numb_of_second_tier_users = second_tier_users_and_company['unique_ties'].str.len().sum()\n",
    "\n",
    "print(numb_of_second_tier_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Instantiating the GithubScraper and scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in scrapelogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File exists: first_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: second_tier_userinfo_user_log.jsonl\n",
      "[INFO] File exists: users_attempted_scrape.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Initialize sets for tracking\n",
    "users_already_scraped = set()\n",
    "companies_already_scraped = set()\n",
    "users_attempted_scraped = set()\n",
    "\n",
    "# Paths\n",
    "first_tier_user_log_file = 'first_tier_userinfo_user_log.jsonl'\n",
    "second_tier_user_log_file = 'second_tier_userinfo_user_log.jsonl'\n",
    "users_attempted_scrape_file = 'users_attempted_scrape.jsonl'\n",
    "\n",
    "first_tier_user_log_path = fp_output_external / first_tier_user_log_file\n",
    "second_tier_user_log_path = fp_output_external / second_tier_user_log_file\n",
    "users_attempted_scrape_path = fp_output_external / users_attempted_scrape_file\n",
    "\n",
    "def ensure_file_exists(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[INFO] File does not exist. Creating: {path.name}\")\n",
    "        path.touch(exist_ok=True)\n",
    "    else:\n",
    "        print(f\"[INFO] File exists: {path.name}\")\n",
    "\n",
    "def load_users_from_log(path: Path):\n",
    "    users = set()\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    user_info = json.loads(line)\n",
    "                    users.add(user_info[\"user_login\"])\n",
    "                except (json.JSONDecodeError, KeyError) as err:\n",
    "                    print(f\"[WARNING] Skipping malformed user line in {path.name}: {err}\")\n",
    "    return users\n",
    "\n",
    "# Ensure all files exist\n",
    "for path in [first_tier_user_log_path, second_tier_user_log_path, users_attempted_scrape_path]:\n",
    "    ensure_file_exists(path)\n",
    "\n",
    "# Populate sets\n",
    "users_already_scraped |= load_users_from_log(first_tier_user_log_path)\n",
    "users_already_scraped |= load_users_from_log(second_tier_user_log_path)\n",
    "users_already_attempted = load_users_from_log(users_attempted_scrape_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiating the GithubScraper and scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GithubScraper initialized with 0 companies and 37 users already scraped.\n",
      "GitHub REST API ratelimit reset time for token is 2025-08-12 21:55:33. That will be in a little less than 38 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4132 [00:00<?, ?user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] User hcarreras already scraped. Skipping.\n",
      "[INFO] User aboedker already scraped. Skipping.\n",
      "[INFO] User adminabtion already scraped. Skipping.\n",
      "[INFO] User martinvintherp already scraped. Skipping.\n",
      "[INFO] User GuldbekLEGO already scraped. Skipping.\n",
      "[INFO] User AngelleAbtion already scraped. Skipping.\n",
      "[INFO] User finnpedersenkazes already scraped. Skipping.\n",
      "[INFO] User karuncs already scraped. Skipping.\n",
      "[INFO] User eemailme already scraped. Skipping.\n",
      "[INFO] User Aberen already scraped. Skipping.\n",
      "[INFO] User reinisla already scraped. Skipping.\n",
      "[INFO] User AskeLange already scraped. Skipping.\n",
      "[INFO] User morgenhaar already scraped. Skipping.\n",
      "[INFO] User nauman already scraped. Skipping.\n",
      "[INFO] User djuric already scraped. Skipping.\n",
      "[INFO] User RobWu already scraped. Skipping.\n",
      "[INFO] User ozf already scraped. Skipping.\n",
      "[INFO] User MikkelHansenAbtion already scraped. Skipping.\n",
      "[INFO] User Kosai106 already scraped. Skipping.\n",
      "[INFO] User bohme already scraped. Skipping.\n",
      "[INFO] User Tejs-Abtion already scraped. Skipping.\n",
      "[INFO] User RassaLibre already scraped. Skipping.\n",
      "[INFO] User mulky-sulaiman already scraped. Skipping.\n",
      "[INFO] User vayurobins already scraped. Skipping.\n",
      "[INFO] User martincarlsen already scraped. Skipping.\n",
      "[INFO] User AlejandraValdivia already scraped. Skipping.\n",
      "[INFO] User JijoBose already scraped. Skipping.\n",
      "[INFO] User bokh already scraped. Skipping.\n",
      "[INFO] User simonask already scraped. Skipping.\n",
      "[INFO] User liljom already scraped. Skipping.\n",
      "[INFO] User andersklenke already scraped. Skipping.\n",
      "[INFO] User MadsZeneli already scraped. Skipping.\n",
      "[INFO] User Dynastig already scraped. Skipping.\n",
      "[INFO] User casiodk already scraped. Skipping.\n",
      "[INFO] User stefan-vrskovy already scraped. Skipping.\n",
      "[INFO] User heatherm already scraped. Skipping.\n",
      "[INFO] User paramadeep already scraped. Skipping.\n",
      "[INFO] User hrithikt already scraped. Skipping.\n",
      "[INFO] User Typeform already scraped. Skipping.\n",
      "[INFO] User viesii already scraped. Skipping.\n",
      "[INFO] User Stevemoretz already scraped. Skipping.\n",
      "[INFO] User sinisterchipmunk already scraped. Skipping.\n",
      "[INFO] User substancelab already scraped. Skipping.\n",
      "[INFO] User dj-abtion already scraped. Skipping.\n",
      "[INFO] User cbit-abtion already scraped. Skipping.\n",
      "[INFO] User pedryvo already scraped. Skipping.\n",
      "[INFO] User cosmic-banana already scraped. Skipping.\n",
      "[INFO] User adionditsak already scraped. Skipping.\n",
      "[INFO] User a-alak already scraped. Skipping.\n",
      "[INFO] User slamidtfyn already scraped. Skipping.\n",
      "[INFO] User iox already scraped. Skipping.\n",
      "[INFO] User thatdoogieguy already scraped. Skipping.\n",
      "[INFO] User allowishus-dev already scraped. Skipping.\n",
      "[INFO] User jekyll already scraped. Skipping.\n",
      "[INFO] User synth already scraped. Skipping.\n",
      "[INFO] User epistrephein already scraped. Skipping.\n",
      "[INFO] User TimMoser92 already scraped. Skipping.\n",
      "[INFO] User MartJohannsen already scraped. Skipping.\n",
      "[INFO] User parseb already scraped. Skipping.\n",
      "[INFO] User runephilosof-abtion already scraped. Skipping.\n",
      "[INFO] User namuit already scraped. Skipping.\n",
      "[INFO] User Guldbek already scraped. Skipping.\n",
      "[INFO] User GeoffAbtion already scraped. Skipping.\n",
      "[INFO] User minhng92 already scraped. Skipping.\n",
      "[INFO] User magnusfriis already scraped. Skipping.\n",
      "[INFO] User jeppester already scraped. Skipping.\n",
      "[INFO] User hellevibeke already scraped. Skipping.\n",
      "[INFO] User knoopx already scraped. Skipping.\n",
      "[INFO] User hacketyhack already scraped. Skipping.\n",
      "[INFO] User fraywing already scraped. Skipping.\n",
      "[INFO] User Citizen2028 already scraped. Skipping.\n",
      "[INFO] User angusshire already scraped. Skipping.\n",
      "[INFO] User IanAbildskou already scraped. Skipping.\n",
      "[INFO] User apneadiving already scraped. Skipping.\n",
      "[INFO] User albertoqa already scraped. Skipping.\n",
      "[INFO] User bureson already scraped. Skipping.\n",
      "[INFO] User aidanogues already scraped. Skipping.\n",
      "[INFO] User kaspernj already scraped. Skipping.\n",
      "[INFO] User afcapel already scraped. Skipping.\n",
      "[INFO] User alexrothenberg already scraped. Skipping.\n",
      "[INFO] User dcabo already scraped. Skipping.\n",
      "[INFO] User dhl already scraped. Skipping.\n",
      "[INFO] User markets already scraped. Skipping.\n",
      "[INFO] User josecolella already scraped. Skipping.\n",
      "[INFO] User PartidoDeInternet already scraped. Skipping.\n",
      "[INFO] User ppruiz already scraped. Skipping.\n",
      "[INFO] User clacloy already scraped. Skipping.\n",
      "[INFO] User juanruiztorres661 already scraped. Skipping.\n",
      "[INFO] User vemv already scraped. Skipping.\n",
      "[INFO] User PragTob already scraped. Skipping.\n",
      "[INFO] User Hobo already scraped. Skipping.\n",
      "[INFO] User RasmusEklund already scraped. Skipping.\n",
      "[INFO] User granadajs already scraped. Skipping.\n",
      "[INFO] User JJ already scraped. Skipping.\n",
      "[INFO] User abtion already scraped. Skipping.\n",
      "[INFO] User toblerone554 already scraped. Skipping.\n",
      "[INFO] User JannikStreek already scraped. Skipping.\n",
      "[INFO] User namankumarjangid already scraped. Skipping.\n",
      "[INFO] User tareksamni already scraped. Skipping.\n",
      "[INFO] User LoneKP already scraped. Skipping.\n",
      "[INFO] User Samu92 already scraped. Skipping.\n",
      "[INFO] User ello already scraped. Skipping.\n",
      "[INFO] User voodoorai2000 already scraped. Skipping.\n",
      "[INFO] User JAntonioMarin already scraped. Skipping.\n",
      "[INFO] User vercel already scraped. Skipping.\n",
      "[INFO] User rogercampos already scraped. Skipping.\n",
      "[INFO] User thoughtbot already scraped. Skipping.\n",
      "[INFO] User Emguma already scraped. Skipping.\n",
      "[INFO] User kostylo already scraped. Skipping.\n",
      "[INFO] User andris9 already scraped. Skipping.\n",
      "[INFO] User Armienn already scraped. Skipping.\n",
      "[INFO] User keja already scraped. Skipping.\n",
      "[INFO] User svenfuchs already scraped. Skipping.\n",
      "[INFO] User blhio already scraped. Skipping.\n",
      "[INFO] User n-singh already scraped. Skipping.\n",
      "[INFO] User jimacoff already scraped. Skipping.\n",
      "[INFO] User fabn already scraped. Skipping.\n",
      "[INFO] User streamio already scraped. Skipping.\n",
      "[INFO] User evisko already scraped. Skipping.\n",
      "[INFO] User bloodybit already scraped. Skipping.\n",
      "[INFO] User cvolund already scraped. Skipping.\n",
      "[INFO] User adrianadaria already scraped. Skipping.\n",
      "[INFO] User bjarkof already scraped. Skipping.\n",
      "[INFO] User balaboon already scraped. Skipping.\n",
      "[INFO] User Quarzi already scraped. Skipping.\n",
      "[INFO] User 10bestdesign already scraped. Skipping.\n",
      "[INFO] User bjslips already scraped. Skipping.\n",
      "[INFO] User bdobry already scraped. Skipping.\n",
      "[INFO] User itsecurityco already scraped. Skipping.\n",
      "[INFO] User neoneye already scraped. Skipping.\n",
      "[INFO] User standardrb already scraped. Skipping.\n",
      "[INFO] User galtzo-floss already scraped. Skipping.\n",
      "[INFO] User jreybert already scraped. Skipping.\n",
      "[INFO] User heroku already scraped. Skipping.\n",
      "[INFO] User roo-rb already scraped. Skipping.\n",
      "[INFO] User catsky already scraped. Skipping.\n",
      "[INFO] User munificent already scraped. Skipping.\n",
      "[INFO] User tridactyl already scraped. Skipping.\n",
      "[INFO] User voormedia already scraped. Skipping.\n",
      "[INFO] User tabler already scraped. Skipping.\n",
      "[INFO] User influitive already scraped. Skipping.\n",
      "[INFO] User tpope already scraped. Skipping.\n",
      "[INFO] User brooklynDev already scraped. Skipping.\n",
      "[INFO] User janko already scraped. Skipping.\n",
      "[INFO] User rubocop already scraped. Skipping.\n",
      "[INFO] User mainmatter already scraped. Skipping.\n",
      "[INFO] User lynndylanhurley already scraped. Skipping.\n",
      "[INFO] User ilyakatz already scraped. Skipping.\n",
      "[INFO] User Insti already scraped. Skipping.\n",
      "[INFO] User Dreamersoul already scraped. Skipping.\n",
      "[INFO] User zipmark already scraped. Skipping.\n",
      "[INFO] User bash-unit already scraped. Skipping.\n",
      "[INFO] User simon0191 already scraped. Skipping.\n",
      "[INFO] User iamthefox already scraped. Skipping.\n",
      "[INFO] User lostisland already scraped. Skipping.\n",
      "[INFO] User heartcombo already scraped. Skipping.\n",
      "[INFO] User mobajo already scraped. Skipping.\n",
      "[INFO] User arshwinth already scraped. Skipping.\n",
      "[INFO] User AI-Agent-Incubator-Month already scraped. Skipping.\n",
      "[INFO] User hallandsen already scraped. Skipping.\n",
      "[INFO] User nila1010 already scraped. Skipping.\n",
      "[INFO] User nelix already scraped. Skipping.\n",
      "[INFO] User Uvacoder already scraped. Skipping.\n",
      "[INFO] User romanIlyushin-bc already scraped. Skipping.\n",
      "[INFO] User andreychev already scraped. Skipping.\n",
      "[INFO] User chrisaspringer already scraped. Skipping.\n",
      "[INFO] User estherfinsen already scraped. Skipping.\n",
      "[INFO] User kristofferkjelde already scraped. Skipping.\n",
      "[INFO] User yenbekbay already scraped. Skipping.\n",
      "[INFO] User sasetkaKmd already scraped. Skipping.\n",
      "[INFO] User zag already scraped. Skipping.\n",
      "[INFO] User rcherny already scraped. Skipping.\n",
      "[INFO] User sebastiantecsi already scraped. Skipping.\n",
      "[INFO] User mhnnovicell already scraped. Skipping.\n",
      "[INFO] User nikitavoloboev already scraped. Skipping.\n",
      "[INFO] User dreijer-dev already scraped. Skipping.\n",
      "[NEW] GitHub ratelimit threshold set to 300 (max rate: 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 177/4132 [00:00<00:09, 421.51user/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WAIT] Remaining requests: 298. Sleeping for 2325.4s until 2025-08-12 21:57:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 177/4132 [00:20<00:09, 421.51user/s]"
     ]
    }
   ],
   "source": [
    "# 1. Create instance of GithubScraper\n",
    "gs = GithubScraper(\n",
    "    users_already_scraped=users_already_scraped,\n",
    "    companies_already_scraped=companies_already_scraped,\n",
    "    users_already_attempted=users_already_attempted,\n",
    "    repo_limit=100\n",
    ")\n",
    "\n",
    "second_tier_users_to_scrape = {\n",
    "    user: row['search_with_company']\n",
    "    for _, row in second_tier_users_and_company.iterrows() # type: ignore\n",
    "    for user in row['unique_ties']\n",
    "}\n",
    "\n",
    "print(f'GitHub REST API ratelimit reset time for token is {gs.reset_time_point}. '\n",
    "      f'That will be in a little less than {gs.reset_time_in_minutes} minutes.')\n",
    "\n",
    "# 2. Define output file name\n",
    "file_name = 'second_tier_userinfo'\n",
    "\n",
    "# 3. Loop through company queries\n",
    "for user, search_with_company in tqdm(second_tier_users_to_scrape.items(), unit=\"user\"):\n",
    "\n",
    "    # 3.3 Check if user is already scraped\n",
    "    if user in gs.users_already_attempted:\n",
    "        print(f'[INFO] User {user} already scraped. Skipping.')\n",
    "        continue\n",
    "\n",
    "    # Log user to the set of already attempted users\n",
    "    gs.log_user_scrape_attempt(user, users_attempted_scrape_path)\n",
    "    gs.users_already_attempted.add(user)\n",
    "\n",
    "    # 3.1 Get user from the flattened dictionary\n",
    "    named_user = gs.get_user(user)\n",
    "\n",
    "    # 3.2 Check if user is None (e.g., if user is not found)\n",
    "    if named_user is None:\n",
    "        continue\n",
    "\n",
    "    # 3.5 Check if user is a relevant user (DK + company)\n",
    "    user_row = gs.get_user_info(named_user, search_with_company, company_filter=False)\n",
    "    if user_row is None:\n",
    "        continue  # Skip user if they don't meet scraping criteria\n",
    "\n",
    "    # 3.3.3 Extract match data\n",
    "    location_match = user_row.matched_location\n",
    "    inferred_company = user_row.inferred_company\n",
    "    matched_company_strings = user_row.matched_company_strings\n",
    "\n",
    "    # 3.3.4 Save user info and log result\n",
    "    gs.save_file(user_row, file_name, remove_existing_file=True)\n",
    "    gs.log_user_w_match(named_user.login, inferred_company, matched_company_strings, location_match, second_tier_user_log_path) #type: ignore\n",
    "    \n",
    "    print(f'[INFO] {gs.USERS_SCRAPED} users scraped so far.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Scouting New Danish App Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Saving the second tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of second-tier users filtered: 3\n"
     ]
    }
   ],
   "source": [
    "# Load in second-tier users\n",
    "with open(fp_output_external / \"second_tier_userinfo.jsonl\", \"r\") as f:\n",
    "    second_tier_users = [json.loads(line) for line in f]\n",
    "\n",
    "# Print number of users\n",
    "print(f\"Number of unique users in dataset: {len(second_tier_users)}\")\n",
    "\n",
    "# Outputting sorted second-tier-user list with gzip (because of list within the dataframe)\n",
    "second_tier_users.to_parquet(\n",
    "    fp_output_external / \"second_tier_ties.parquet.gzip\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Make a filtered copy of second-tier users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy to subset\n",
    "second_tier_users_subset = second_tier_users.copy()\n",
    "\n",
    "# Making a company mask, indicating whether there is an entry or not in the GitHub location variable\n",
    "company_mask = [\n",
    "    bool(user.get(\"listed_company\"))\n",
    "    for user in second_tier_users_subset\n",
    "]\n",
    "\n",
    "# Subset the second tier users based on the company mask\n",
    "second_tier_users_filtered = [\n",
    "    user for user, keep in zip(second_tier_users_subset, company_mask) if keep\n",
    "]\n",
    "\n",
    "print(f\"Number of second-tier users filtered: {len(second_tier_users_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Output for manual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE ON MANUAL INSPECTION**\n",
    "\n",
    "The outputted file \"second_tier_users_filtered_subset.csv\" will be manually gone through in a csv-reader program. From this a list of second-tier companies, which we choose to include in our sample, will be produced and found in the file: \".resources/second_tier_companies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset on bio variables to output for manual inspection of new companies\n",
    "variables = [\"user_login\", \"listed_company\", \"inferred_company\", \"email\", \"bio\", \"blog\", \"github_location\", \"matched_location\"]\n",
    "\n",
    "# Create a filtered dataset for manual inspection\n",
    "second_tier_users_filtered_subset = [\n",
    "    {var: user.get(var) for var in variables}\n",
    "    for user in second_tier_users_filtered\n",
    "]\n",
    "\n",
    "# Create empty column to input new companies.\n",
    "for user in second_tier_users_filtered_subset:\n",
    "    user[\"new_company\"] = None\n",
    "\n",
    "# Output the subset for review as csv\n",
    "df = pd.DataFrame(second_tier_users_filtered_subset)\n",
    "df.to_csv(fp_main_output / \"second_tier_users_filtered_subset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
